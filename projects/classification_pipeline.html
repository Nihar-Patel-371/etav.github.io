<!DOCTYPE html>
<html lang="en">

<head>
      <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="Applied Data Science, progamming and machine learning projects">
    <meta name="author" content="Ernest Tavares">
    <link rel="icon" href="../favicon.ico">

    <title>Classifying Charity Donors - Projects</title>

    <!-- JQuery -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script>
        window.jQuery || document.write('<script src="../theme/js/jquery.min.js"><\/script>')
    </script>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="../theme/css/bootstrap.css" />
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link rel="stylesheet" type="text/css" href="../theme/css/ie10-viewport-bug-workaround.css" />
    <!-- Custom styles for this template -->
    <link rel="stylesheet" type="text/css" href="../theme/css/style.css" />
    <link rel="stylesheet" type="text/css" href="../theme/css/notebooks.css" />
    <link href='https://fonts.googleapis.com/css?family=PT+Serif:400,700|Roboto:400,500,700' rel='stylesheet' type='text/css'>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
     <link href="https://etav.github.io/feeds/all.rss.xml" type="application/rss+xml" rel="alternate" title="Ernest Tavares III Full RSS Feed" />        



</head>

<body>

    <div class="navbar navbar-fixed-top">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="..">Ernest Tavares III</a>
            </div>
            <div class="navbar-collapse collapse" id="searchbar">

                <ul class="nav navbar-nav navbar-right">
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">About<span class="caret"></span></a>
                        <ul class="dropdown-menu">
                            <li><a href="../pages/about.html">About Ernest</a></li>
                            <li><a href="https://www.linkedin.com/in/ernesttavares">LinkedIn</a></li>
                            <li><a href="https://twitter.com/etav3">Twitter</a></li>
                            <li><a href="https://github.com/etav/">GitHub</a></li>
                            <li><a href="https://www.instagram.com/nomad94/">Instagram</a></li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Data Science<span class="caret"></span></a>
                        <ul class="dropdown-menu">
                            <li><a href="..#Python">Python</a></li>
                            <li><a href="..#R_Stats">R Stats</a></li>
                            <li><a href="..#Machine_Learning">Machine Learning</a></li>
                            <li><a href="..#Articles">Articles</a></li>
                            <li><a href="..#Projects">Projects</a></li>
                        </ul>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Projects<span class="caret"></span></a>
                        <ul class="dropdown-menu">
                            <li><a href="https://github.com/etav/animal_shelter">Animal Shelter + Random Forest</a></li>
                        </ul>
                    </li>

                    <li class="dropdown">
                        <a href="../feeds/all.rss.xml">Feed</a>
                    </li>


                </ul>

                <form class="navbar-form" action="../search.html" onsubmit="return validateForm(this.elements['q'].value);">
                    <div class="form-group" style="display:inline;">
                        <div class="input-group" style="display:table;">
                            <span class="input-group-addon" style="width:1%;"><span class="glyphicon glyphicon-search"></span></span>
                            <input class="form-control search-query" name="q" id="tipue_search_input" placeholder="e.g. scikit KNN, pandas merge" required autocomplete="off" type="text">
                        </div>
                    </div>
                </form>

            </div>
            <!--/.nav-collapse -->
        </div>
    </div>



    <!-- end of header section -->
    <div class="container">
<!-- <div class="alert alert-warning" role="alert">
    Did you find this page useful? Please do me a quick favor and <a href="#" class="alert-link">endorse me for data science on LinkedIn</a>.
</div> -->
<section id="content" class="body">
    <header>
    <h1>
      Classifying Charity Donors
    </h1>
<ol class="breadcrumb">
    <li>
        <time class="published" datetime="2017-07-23T12:20:00-07:00">
            23 July 2017
        </time>
    </li>
    <li>Projects</li>
</ol>
</header>
<div class='article_content'>
<h2>Classifying Charity Donors</h2>
<p>In this project, I will employed several supervised algorithms to accurately model individuals' income using data collected from the 1994 U.S. Census. I then identified the best candidate algorithm from preliminary results and further optimized this algorithm to best model the data.</p>
<p>The goal of this analysis is to construct a model that accurately predicts whether an individual makes more than $50,000 USD annually. This sort of task can is common in a non-profit settings, where organizations survive on donations.  </p>
<p>Armed with information on an individual's income level aids non-profits in understanding how large of a donation to request, or whether they should contact certain individuals begin with. While it can be difficult to determine an individual's general income, we can (as we will see) infer this value from other publicly available features such as government census studies.</p>
<p>The dataset for this project originates from the <a href="https://archive.ics.uci.edu/ml/datasets/Census+Income">UCI Machine Learning Repository</a>. The datset was donated by Ron Kohavi and Barry Becker, after being published in the article <em>"Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid"</em>. You can find the article by Ron Kohavi <a href="https://www.aaai.org/Papers/KDD/1996/KDD96-033.pdf">online</a>. The data we investigate here consists of small changes to the original dataset, such as removing the <code>'fnlwgt'</code> feature and records with missing or ill-formatted entries.</p>
<p><em>I completed this project as part of the <a href="https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009">Udacity Machine Learning Engineer Nano Degree</a></em></p>
<div class="highlight"><pre><span></span><span class="c1"># Import libraries necessary for this project</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span> <span class="c1"># Allows the use of display() for DataFrames</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span> <span class="c1">#for division</span>


<span class="c1"># Import supplementary visualization code</span>
<span class="kn">import</span> <span class="nn">visuals</span> <span class="kn">as</span> <span class="nn">vs</span>

<span class="c1"># Pretty display for notebooks</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1"># Load the Census dataset</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;census.csv&quot;</span><span class="p">)</span>

<span class="c1"># Success - Display the first record</span>
<span class="n">display</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</pre></div>


<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>workclass</th>
      <th>education_level</th>
      <th>education-num</th>
      <th>marital-status</th>
      <th>occupation</th>
      <th>relationship</th>
      <th>race</th>
      <th>sex</th>
      <th>capital-gain</th>
      <th>capital-loss</th>
      <th>hours-per-week</th>
      <th>native-country</th>
      <th>income</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>39</td>
      <td>State-gov</td>
      <td>Bachelors</td>
      <td>13</td>
      <td>Never-married</td>
      <td>Adm-clerical</td>
      <td>Not-in-family</td>
      <td>White</td>
      <td>Male</td>
      <td>2174</td>
      <td>0</td>
      <td>40</td>
      <td>United-States</td>
      <td>&lt;=50K</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="c1">#summary stats</span>
<span class="n">data</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
</pre></div>


<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>education-num</th>
      <th>capital-gain</th>
      <th>capital-loss</th>
      <th>hours-per-week</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>45222</td>
      <td>45222</td>
      <td>45222</td>
      <td>45222</td>
      <td>45222</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>38</td>
      <td>10</td>
      <td>1101</td>
      <td>88</td>
      <td>40</td>
    </tr>
    <tr>
      <th>std</th>
      <td>13</td>
      <td>2</td>
      <td>7506</td>
      <td>404</td>
      <td>12</td>
    </tr>
    <tr>
      <th>min</th>
      <td>17</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>28</td>
      <td>9</td>
      <td>0</td>
      <td>0</td>
      <td>40</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>37</td>
      <td>10</td>
      <td>0</td>
      <td>0</td>
      <td>40</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>47</td>
      <td>13</td>
      <td>0</td>
      <td>0</td>
      <td>45</td>
    </tr>
    <tr>
      <th>max</th>
      <td>90</td>
      <td>16</td>
      <td>99999</td>
      <td>4356</td>
      <td>99</td>
    </tr>
  </tbody>
</table>
</div>

<h3>Data Exploration</h3>
<p>As a preliminary step let's determine how many individuals within our data set fall into each of our two buckets:
- Individuals making more than \$50K annually
- Individuals making less than \$50K annually</p>
<p>Also bear in mind that this census data was from 1994 and annual wages have inflated since then.</p>
<div class="highlight"><pre><span></span><span class="c1"># Total number of records</span>
<span class="n">n_records</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Number of records where individual&#39;s income is more than $50,000</span>
<span class="n">n_greater_50k</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">income</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="c1"># Number of records where individual&#39;s income is at most $50,000</span>
<span class="n">n_at_most_50k</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">income</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="c1"># Percentage of individuals whose income is more than $50,000</span>
<span class="n">greater_percent</span> <span class="o">=</span> <span class="n">n_greater_50k</span> <span class="o">/</span> <span class="n">n_records</span>

<span class="c1"># Print the results</span>
<span class="k">print</span> <span class="s2">&quot;Total number of records: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_records</span><span class="p">)</span>
<span class="k">print</span> <span class="s2">&quot;Individuals making more than $50,000: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_greater_50k</span><span class="p">)</span>
<span class="k">print</span> <span class="s2">&quot;Individuals making at most $50,000: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_at_most_50k</span><span class="p">)</span>
<span class="k">print</span> <span class="s2">&quot;Percentage of individuals making more than $50,000: {:.2f}%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">greater_percent</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Total number of records: 45222
Individuals making more than $50,000: 11208
Individuals making at most $50,000: 34014
Percentage of individuals making more than $50,000: 24.78%
</pre></div>


<h2>Data Pre-Processing</h2>
<p>Before we move on to modeling we're going to perform some preprocessing on our dataset to adjust the quality of our variables. For example, since we’re dealing with a monetary response variable <code>income</code>, it’s common to perform log transformations to normalize its distribution.</p>
<div class="highlight"><pre><span></span><span class="c1"># Split the data into features and target label</span>
<span class="n">income_raw</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;income&#39;</span><span class="p">]</span>
<span class="n">features_raw</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;income&#39;</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Visualize skewed continuous features of original data</span>
<span class="n">vs</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>


<p><img alt="unlogged" src="/images/classification_pipeline/unlogged.png"></p>
<h3>Log Transformation</h3>
<p>Notice the <strong>strong positive skew present</strong> in the capital-gain and capital-loss features. In order to compress the range of our dataset and deal with outliers we will perform a log transformation using <code>np.log()</code>. However, it's important to remember that the log of zero is undefined so we will add 1 to each sample.</p>
<div class="highlight"><pre><span></span><span class="c1"># Log-transform the skewed features</span>
<span class="n">skewed</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;capital-gain&#39;</span><span class="p">,</span> <span class="s1">&#39;capital-loss&#39;</span><span class="p">]</span>
<span class="n">features_raw</span><span class="p">[</span><span class="n">skewed</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">skewed</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="c1">#add 1</span>

<span class="c1"># Visualize the new log distributions</span>
<span class="n">vs</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="n">features_raw</span><span class="p">,</span> <span class="n">transformed</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
</pre></div>


<p><img alt="logged" src="/images/classification_pipeline/logged.png"></p>
<p>The new distribution, which is still non-normally distributed, is much better than our initial state. This effect is more pronounced on the <strong>capital-gain feature</strong>.</p>
<h3>Scaling (Normalizing) Numeric Features</h3>
<p>After implementing our log transformation, it's good practice to perform scaling on numerical features so that each feature will be weighted equally when we have our algorithm ingest it. <strong>NOTE: once scaling has been applied, the features will not be recognizable.</strong></p>
<p>To do this we'll employ sklearn's <a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"><code>sklearn.preprocessing.MinMaxScaler</code></a>. Any outliers will dramatically effect the results of the scaling, that's why we handled them with a log transformation in the previous step. What's happening under-the-hood of this function is a simple division and subtraction to re-weight each sample within each feature such that they all fall within the range (0,1). The math behind the function is displayed below:</p>
<div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">latex</span>

<span class="err">$$</span>\ <span class="n">x_</span><span class="p">{</span><span class="n">scaled</span><span class="p">}</span><span class="o">=</span>\<span class="n">frac</span><span class="p">{</span><span class="n">X</span> <span class="o">-</span> <span class="n">X_</span><span class="p">{</span><span class="nb">min</span><span class="p">}}{</span><span class="n">X_</span><span class="p">{</span><span class="nb">max</span><span class="p">}</span> <span class="o">-</span> <span class="n">X_</span><span class="p">{</span><span class="nb">min</span><span class="p">}}</span> <span class="err">$$</span>
</pre></div>


<p>$$\ x_{scaled}=\frac{X - X_{min}}{X_{max} - X_{min}} $$</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>

<span class="c1"># Initialize a scaler, then apply it to the features</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">numerical</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;education-num&#39;</span><span class="p">,</span> <span class="s1">&#39;capital-gain&#39;</span><span class="p">,</span> <span class="s1">&#39;capital-loss&#39;</span><span class="p">,</span> <span class="s1">&#39;hours-per-week&#39;</span><span class="p">]</span>
<span class="n">features_raw</span><span class="p">[</span><span class="n">numerical</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">numerical</span><span class="p">])</span>

<span class="c1"># Show an example of a record with scaling applied</span>
<span class="n">display</span><span class="p">(</span><span class="n">features_raw</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</pre></div>


<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>workclass</th>
      <th>education_level</th>
      <th>education-num</th>
      <th>marital-status</th>
      <th>occupation</th>
      <th>relationship</th>
      <th>race</th>
      <th>sex</th>
      <th>capital-gain</th>
      <th>capital-loss</th>
      <th>hours-per-week</th>
      <th>native-country</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.30137</td>
      <td>State-gov</td>
      <td>Bachelors</td>
      <td>0.8</td>
      <td>Never-married</td>
      <td>Adm-clerical</td>
      <td>Not-in-family</td>
      <td>White</td>
      <td>Male</td>
      <td>0.02174</td>
      <td>0</td>
      <td>0.397959</td>
      <td>United-States</td>
    </tr>
  </tbody>
</table>
</div>

<h3>Encoding Categorical Features</h3>
<p>Now that the numeric features have been transformed and scaled, it's time to give our categorical features some love. Since most machine learning algorithms will expect all features to be numeric, we'll perform feature encoding using pandas <code>pd.get_dummies()</code> function which will transform category values into numeric dummy variables within a dataframe.</p>
<p>We'll also encode the the response variable (income) with 0 = income less than \$50K and 1 = income greater than \$50K.</p>
<div class="highlight"><pre><span></span><span class="c1"># Encode categorical features</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">features_raw</span><span class="p">)</span>

<span class="c1"># Encode the &#39;income_raw&#39; data to numerical values</span>
<span class="n">income</span> <span class="o">=</span> <span class="n">income_raw</span><span class="o">.</span><span class="n">replace</span><span class="p">({</span> <span class="s1">&#39;&lt;=50K&#39;</span> <span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;&gt;50K&#39;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">})</span>

<span class="c1"># Print the number of features after one-hot encoding</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="k">print</span> <span class="s2">&quot;{} total features after encoding.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">encoded</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>103 total features after encoding.
</pre></div>


<h3>Shuffle Split the Data</h3>
<p>Now that we've transformed and scaled our numeric features and encoded both our categorical features as well as our response variable we're ready to split our data set into training and testing sets.</p>
<div class="highlight"><pre><span></span><span class="c1"># Import train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Split the &#39;features&#39; and &#39;income&#39; data into training and testing sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">income</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Show the results of the split</span>
<span class="k">print</span> <span class="s2">&quot;Training set has {} samples.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span> <span class="s2">&quot;Testing set has {} samples.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>


<div class="highlight"><pre><span></span>Training set has 36177 samples.
Testing set has 9045 samples.
</pre></div>


<h3>Establishing a Baseline Model</h3>
<p><em>If we create a mode always predicted an individual made more than \$50,000, what would that model's accuracy and F-score be on this dataset?</em>  </p>
<p>Let's test this out to serve as a baseline to compare our other learning algorithms's performance.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1">#naive classifier</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">income</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">#confusion matrix</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">income</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>

<span class="c1">#metrics</span>
<span class="n">tn</span> <span class="o">=</span> <span class="n">cm</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">fp</span> <span class="o">=</span> <span class="n">cm</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">fn</span> <span class="o">=</span> <span class="n">cm</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">tp</span> <span class="o">=</span> <span class="n">cm</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>

<span class="n">recall</span> <span class="o">=</span> <span class="n">tp</span> <span class="o">/</span> <span class="p">(</span><span class="n">tp</span> <span class="o">+</span> <span class="n">fn</span><span class="p">)</span>
<span class="n">precision</span> <span class="o">=</span> <span class="n">tp</span> <span class="o">/</span> <span class="p">(</span><span class="n">tp</span> <span class="o">+</span> <span class="n">fp</span><span class="p">)</span>

<span class="n">beta</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span>

<span class="c1"># Calculate accuracy</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">tp</span> <span class="o">+</span> <span class="n">tn</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">tp</span> <span class="o">+</span> <span class="n">tn</span> <span class="o">+</span> <span class="n">fp</span> <span class="o">+</span> <span class="n">fn</span><span class="p">)</span>

<span class="c1"># Calculate F-score using the formula above for beta = 0.5</span>

<span class="n">fscore</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">beta</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">precision</span> <span class="o">*</span> <span class="n">recall</span><span class="p">)</span> <span class="o">/</span> <span class="p">((</span><span class="n">beta</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">precision</span><span class="p">)</span> <span class="o">+</span> <span class="n">recall</span><span class="p">))</span>

<span class="c1"># Print the results</span>
<span class="k">print</span> <span class="s2">&quot;Naive Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">fscore</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">Naive</span> <span class="nl">Predictor</span><span class="p">:</span> <span class="p">[</span><span class="n">Accuracy</span> <span class="nl">score</span><span class="p">:</span> <span class="mf">0.2478</span><span class="p">,</span> <span class="n">F</span><span class="o">-</span><span class="nl">score</span><span class="p">:</span> <span class="mf">0.2917</span><span class="p">]</span>
</pre></div>


<p>As expected, the Naive Predictor's accuracy is equal to the number of individuals making over \$50K. Let's see if we can create a model which does a better job at predicting!</p>
<h3>Classification &amp; Training — Creating a Modeling Pipeline</h3>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">fbeta_score</span> <span class="c1">#metrics / scoring</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>

<span class="k">def</span> <span class="nf">train_predict</span><span class="p">(</span><span class="n">learner</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    inputs:</span>
<span class="sd">       - learner: the learning algorithm to be trained and predicted on</span>
<span class="sd">       - sample_size: the size of samples (number) to be drawn from training set</span>
<span class="sd">       - X_train: features training set</span>
<span class="sd">       - y_train: income training set</span>
<span class="sd">       - X_test: features testing set</span>
<span class="sd">       - y_test: income testing set</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Fit the learner to the training data using slicing with &#39;sample_size&#39;</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span> <span class="c1"># Get start time</span>
    <span class="n">learner</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:</span><span class="n">sample_size</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[:</span><span class="n">sample_size</span><span class="p">])</span> <span class="c1">#sample_weight=sample_size</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span> <span class="c1"># Get end time</span>

    <span class="c1"># Calculate the training time</span>
    <span class="n">results</span><span class="p">[</span><span class="s1">&#39;train_time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

    <span class="c1"># Get the predictions on the test set,</span>
    <span class="c1">#       then get predictions on the first 300 training samples</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span> <span class="c1"># Get start time</span>
    <span class="n">predictions_test</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span> <span class="c1">#pred = clf.predict(features_test)</span>
    <span class="n">predictions_train</span> <span class="o">=</span> <span class="n">learner</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:</span><span class="mi">300</span><span class="p">])</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span> <span class="c1"># Get end time</span>

    <span class="c1"># Total prediction time</span>
    <span class="n">results</span><span class="p">[</span><span class="s1">&#39;pred_time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

    <span class="c1"># Compute accuracy on 300 training samples</span>
    <span class="n">results</span><span class="p">[</span><span class="s1">&#39;acc_train&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">[:</span><span class="mi">300</span><span class="p">],</span> <span class="n">predictions_train</span><span class="p">)</span>

    <span class="c1"># Compute accuracy on test set</span>
    <span class="n">results</span><span class="p">[</span><span class="s1">&#39;acc_test&#39;</span><span class="p">]</span>  <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions_test</span><span class="p">)</span>


    <span class="c1"># Compute F-score on 300 training samples</span>
    <span class="n">results</span><span class="p">[</span><span class="s1">&#39;f_train&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fbeta_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">[:</span><span class="mi">300</span><span class="p">],</span> <span class="n">predictions_train</span><span class="p">,</span> <span class="n">beta</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>


    <span class="c1"># Compute F-score on the test set</span>
    <span class="n">results</span><span class="p">[</span><span class="s1">&#39;f_test&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fbeta_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions_test</span><span class="p">,</span> <span class="n">beta</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>

    <span class="c1"># Success</span>
    <span class="k">print</span> <span class="s2">&quot;{} trained on {} samples.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">learner</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">)</span>

    <span class="c1"># Return the results</span>
    <span class="k">return</span> <span class="n">results</span>

<span class="n">train_predict</span><span class="p">(</span><span class="n">GaussianNB</span><span class="p">(),</span> <span class="mi">36177</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>GaussianNB trained on 36177 samples.





{&#39;acc_test&#39;: 0.60829187396351581,
 &#39;acc_train&#39;: 0.59333333333333338,
 &#39;f_test&#39;: 0.42811288507232487,
 &#39;f_train&#39;: 0.41249999999999998,
 &#39;pred_time&#39;: 0.028007984161376953,
 &#39;train_time&#39;: 0.12101292610168457}
</pre></div>


<h3>Selecting Algorithm Candidates</h3>
<table>
<thead>
<tr>
<th>Application Example</th>
<th>Strengths</th>
<th>Weaknesses</th>
<th>Why this is a good model for this specific problem</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Gaussian Naive Bayes (GaussianNB)</strong> - "Naive Bayes spam filtering is a baseline technique for dealing with spam that can tailor itself to the email needs of individual users and give low false positive spam detection rates that are generally acceptable to users. It is one of the oldest ways of doing spam filtering, with roots in the 1990s." <a href="https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering">Source</a></td>
<td><ul><li>Requires a small amount of training data to train an algorithm</li><li>Extremely quick training time</li></td>
<td><ul><li>Decent at classifying but bad at predicting</li></td>
<td>Since we're trying to bucket individuals into income brackets NB's algorithm is a good candidate. If we were trying to predict an individual's exact income I would be less inclined to use this algorithm.</td>
</tr>
<tr>
<td><strong>Logistic Regression</strong> - "Logistic regression is used in various fields, including machine learning, most medical fields, and social sciences."  <br> It lends itself particularly well to binary classification tasks common in medical research, such as classifying whether someone has a certain illness or not."For example, the Trauma and Injury Severity Score (TRISS), which is widely used to predict mortality in injured patients, was originally developed by Boyd et al. using logistic regression." <a href="https://www.ncbi.nlm.nih.gov/pubmed/3106646">Source</a></td>
<td><ul><li>High interpretability </li> <li>No parameter tuning necessary</li> <li>Requires a small amount of training data to train an algorithm</li></td>
<td><ul><li>Because Logistic Regression is such an interpretable model, it is often subject to high bias.</li></td>
<td>Logistic Regression is one of the most basic flavors of classification algorithms and since that's exactly the task at hand it appears to be a good candidate to test.</td>
</tr>
<tr>
<td><strong> Random Forest</strong> - "Or random decision forests[1][2] are an ensemble learning method for classification, that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set." Random Forests are one of the most flexible and elegant ML algorithms and they have been applied to a wide range of fields including predicting the quality of surveys using NLP <a href="http://sqp.upf.edu/">Source</a></td>
<td><ul><li>Relatively quick to train</li> <li>Excellent at predicting/classifying on all types of data without overfitting</li></td>
<td><ul><li>Slowest to run compared to other two methods</li> <li>Black Box model meaning we can't really see what's happening under the hood</li></td>
<td>Random Forests algorithm is the bread and butter for many machine learning tasks due to its flexibility and ability to correct individual decision trees  proneness to overfitting by sampling many of them.   I hypothesize this algorithm will out perform the other two.</td>
</tr>
</tbody>
</table>
<h2>Model Comparison / Evaluation</h2>
<p>Great now that we've estabilished a modeling pipeline, established baseline performance and selecte two other algorithms we can now, finally, get to modeling.</p>
<div class="highlight"><pre><span></span><span class="c1"># Import the three supervised learning models from sklearn</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="n">ExtraTreesClassifier</span>

<span class="c1">#2 creating classifier</span>
<span class="c1"># Initialize the three models</span>
<span class="n">clf_A</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">clf_B</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">clf_C</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>


<span class="c1"># Calculate the number of samples for 1%, 10%, and 100% of the training data</span>
<span class="n">samples_1</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mf">0.01</span><span class="p">;</span> <span class="n">samples_1</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">samples_1</span><span class="p">)</span>
<span class="n">samples_10</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mf">0.1</span><span class="p">;</span> <span class="n">samples_10</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">samples_10</span><span class="p">)</span>
<span class="n">samples_100</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span> <span class="n">samples_100</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">samples_100</span><span class="p">)</span>

<span class="c1"># Collect results on the learners</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">clf</span> <span class="ow">in</span> <span class="p">[</span><span class="n">clf_A</span><span class="p">,</span> <span class="n">clf_B</span><span class="p">,</span> <span class="n">clf_C</span><span class="p">]:</span>
    <span class="n">clf_name</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
    <span class="n">results</span><span class="p">[</span><span class="n">clf_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">samples</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">samples_1</span><span class="p">,</span> <span class="n">samples_10</span><span class="p">,</span> <span class="n">samples_100</span><span class="p">]):</span>
        <span class="n">results</span><span class="p">[</span><span class="n">clf_name</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> \
        <span class="n">train_predict</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">samples</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="c1"># Run metrics visualization for the three supervised learning models chosen</span>
<span class="n">vs</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">fscore</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>GaussianNB trained on 361 samples.
GaussianNB trained on 3617 samples.
GaussianNB trained on 36177 samples.
LogisticRegression trained on 361 samples.
LogisticRegression trained on 3617 samples.
LogisticRegression trained on 36177 samples.
RandomForestClassifier trained on 361 samples.
RandomForestClassifier trained on 3617 samples.
RandomForestClassifier trained on 36177 samples.
</pre></div>


<p><img alt="model_comparison" src="/images/classification_pipeline/model_comparison.png"></p>
<h3>Model Selection</h3>
<p>If we care most about the accuracy and speed of our classifier then based on our analysis of the three learning algorithms tested, I recommend we move forward with <strong>Logistic Regression because it outperforms both Naive Bayes and Random Forest in terms of prediction accuracy on our testing set and is much faster to train than random forest.</strong> Furthermore, it's F-score falls between the other two algorithms meaning we'd expect it to fall in the middle of our other two candidates on precision and recall.</p>
<h3><strong>LOGISTIC REGRESSION</strong></h3>
<ul>
<li>What's happening under-the-hood of <strong>logit regression</strong>?</li>
</ul>
<p>We can think of a logit regression as being similar to a linear regression but with a discrete as opposed to continuous output. The classification algorithm draws an S-shaped decision boundary which, in our case, divides the two classes. For each sample within our dataset, the algorithm assigns a probability to determine which class the point would fall into. For example if we have two classes <code>(0 = less than $50K   &amp; 1 = more than $50K)</code>, then (without any tuning of the decision boundary) points with a probability of less than .5 would be classified as 0 and points with greater than .5 probability would be classified as 1.</p>
<h3>TL;DR: In this scenario Logistic Regression provides a model that is more accurate, easy to interpret and quicker to predict on our testing data.</h3>
<h2>Summary So Far</h2>
<p>We rigorously tested three machine learning algorithms to determine which would predict a person's level of income and after our analysis we feel confident in moving forward with logistic regression because it provides high accuracy, ease of interpretability (we get coefficients for each predictor) and is able to quickly provide predictions.</p>
<p>In order to get this far we split our data into training and testing sets. But prior to this we normalized our targets by performing a log transformation on numeric target data, min-max scaling on all numeric predictor variables and one-hot-encoding on all categorical predictor features.</p>
<p>We then trained our models using different cuts of training data (1%, 10% and 100% of the sets) and at each cut logistic regression provided the most accurate results. After the model was trained we used our testing test to determine accuracy and F-scores of each model on data it had not been previously trained on. At this step Logistic regression fell in the middle of the other two algorithms in terms of precision and recall, meaning it had the most neutral bias-variance trade-off.</p>
<h2>Model Tuning</h2>
<p>Not that we've selected Logistic Regression as our model of choice, we can use sklearn's <code>GridSearchCV</code> function to tune our model using different permutations of parameters to our model</p>
<div class="highlight"><pre><span></span><span class="c1"># Import &#39;GridSearchCV&#39;, &#39;make_scorer&#39;, and any other necessary libraries</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">fbeta_score</span><span class="p">,</span> <span class="n">make_scorer</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span><span class="p">,</span> <span class="n">LogisticRegressionCV</span>
<span class="kn">from</span> <span class="nn">sklearn.grid_search</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="c1"># Initialize the classifier</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>

<span class="c1"># Create the parameters list you wish to tune</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span><span class="s2">&quot;solver&quot;</span> <span class="p">:</span> <span class="p">[</span><span class="s1">&#39;newton-cg&#39;</span><span class="p">,</span><span class="s1">&#39;liblinear&#39;</span><span class="p">]}]</span>

<span class="c1"># Make an fbeta_score scoring object</span>
<span class="n">scorer</span> <span class="o">=</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">fbeta_score</span><span class="p">,</span> <span class="n">beta</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>


<span class="c1"># Perform grid search on the classifier using &#39;scorer&#39; as the scoring method</span>
<span class="n">grid_obj</span> <span class="o">=</span>  <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span><span class="n">parameters</span> <span class="p">,</span><span class="n">scoring</span><span class="o">=</span><span class="n">scorer</span><span class="p">)</span>


<span class="c1"># Fit the grid search object to the training data and find the optimal parameters</span>
<span class="n">grid_fit</span> <span class="o">=</span> <span class="n">grid_obj</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>


<span class="c1"># Get the estimator</span>
<span class="n">best_clf</span> <span class="o">=</span> <span class="n">grid_fit</span><span class="o">.</span><span class="n">best_estimator_</span>

<span class="c1"># Make predictions using the unoptimized and model</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">best_predictions</span> <span class="o">=</span> <span class="n">best_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Report the before-and-afterscores</span>
<span class="k">print</span> <span class="s2">&quot;Unoptimized model</span><span class="se">\n</span><span class="s2">------&quot;</span>
<span class="k">print</span> <span class="s2">&quot;Accuracy score on testing data: {:.4f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">))</span>
<span class="k">print</span> <span class="s2">&quot;F-score on testing data: {:.4f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">fbeta_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="k">print</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Optimized Model</span><span class="se">\n</span><span class="s2">------&quot;</span>
<span class="k">print</span> <span class="s2">&quot;Final accuracy score on the testing data: {:.4f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">best_predictions</span><span class="p">))</span>
<span class="k">print</span> <span class="s2">&quot;Final F-score on the testing data: {:.4f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">fbeta_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">best_predictions</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>Unoptimized model
------
Accuracy score on testing data: 0.8483
F-score on testing data: 0.6993

Optimized Model
------
Final accuracy score on the testing data: 0.8498
Final F-score on the testing data: 0.7018
</pre></div>


<h3>Final Model Evaluation</h3>
<h4>Logistic Regression Optimized Results vs Naive Predictor's Baseline Results:</h4>
<table>
<thead>
<tr>
<th align="center">Metric</th>
<th align="center">Benchmark Predictor</th>
<th align="center">Unoptimized Model</th>
<th align="center">Optimized Model</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Accuracy Score</td>
<td align="center">24.38%</td>
<td align="center">84.83%</td>
<td align="center">84.94%</td>
</tr>
<tr>
<td align="center">F-score</td>
<td align="center">.2442</td>
<td align="center">.6993</td>
<td align="center">.7008</td>
</tr>
</tbody>
</table>
<p>As we can see from the comparison table above, our logistic model does a much better job at accurately predicting an individual's income and has a higher F-Score which means it does a better job of balancing precision and recall.</p>
<h2>Extracting Feature Importance</h2>
<p>Here we'll choose a different <code>scikit-learn</code> supervised learning algorithm that has a <code>feature_importance_</code> attribute. This attribute is a function that ranks the importance of each feature when making predictions based on the chosen algorithm so we can gain an understanding of the underlying importance for each feature within our model.</p>
<div class="highlight"><pre><span></span><span class="c1"># Import a supervised learning model that has &#39;feature_importances_&#39;</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>

<span class="c1"># Train the supervised model on the training set</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Extract the feature importances</span>
<span class="n">importances</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span>

<span class="c1"># Plot</span>
<span class="n">vs</span><span class="o">.</span><span class="n">feature_plot</span><span class="p">(</span><span class="n">importances</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>


<p><img alt="feature_compression" src="/images/classification_pipeline/feature_compression.png"></p>
<p>Interestingly capital-gain/loss, age, hours-per-week and education explain the most variane within the dataset.</p>
<p>Also interesting to note that the fives features here together account for ~60% of the total weight given to all predictors.</p>
<h3>Feature Selection</h3>
<p>How does the model perform if we only use a subset of all the available features in the data? With less features required to train, the time requird for training and prediction time is much lower — at the cost of performance metrics.</p>
<p>From the viz above, we see that the top five most important features explain about ~60% of the variance within the dataset. This means that we can potentially <em>compress our feature space</em> and simplify the information required for the model to learn. This code will compare our optimized model with a full feature set to a new optimized model using only the top 5 features.</p>
<div class="highlight"><pre><span></span><span class="c1"># Import functionality for cloning a model</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">clone</span>

<span class="c1"># Reduce the feature space</span>
<span class="n">X_train_reduced</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">[(</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">])[:</span><span class="mi">5</span><span class="p">]]]</span>
<span class="n">X_test_reduced</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">X_test</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">[(</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">])[:</span><span class="mi">5</span><span class="p">]]]</span>

<span class="c1"># Train on the &quot;best&quot; model found from grid search earlier</span>
<span class="n">clf</span> <span class="o">=</span> <span class="p">(</span><span class="n">clone</span><span class="p">(</span><span class="n">best_clf</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_reduced</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Make new predictions</span>
<span class="n">reduced_predictions</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_reduced</span><span class="p">)</span>

<span class="c1"># Report scores from the final model using both versions of data</span>
<span class="k">print</span> <span class="s2">&quot;Final Model trained on full data</span><span class="se">\n</span><span class="s2">------&quot;</span>
<span class="k">print</span> <span class="s2">&quot;Accuracy on testing data: {:.4f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">best_predictions</span><span class="p">))</span>
<span class="k">print</span> <span class="s2">&quot;F-score on testing data: {:.4f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">fbeta_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">best_predictions</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="k">print</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Final Model trained on reduced data</span><span class="se">\n</span><span class="s2">------&quot;</span>
<span class="k">print</span> <span class="s2">&quot;Accuracy on testing data: {:.4f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">reduced_predictions</span><span class="p">))</span>
<span class="k">print</span> <span class="s2">&quot;F-score on testing data: {:.4f}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">fbeta_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">reduced_predictions</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=.</span><span class="mi">5</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>Final Model trained on full data
------
Accuracy on testing data: 0.8498
F-score on testing data: 0.7018

Final Model trained on reduced data
------
Accuracy on testing data: 0.8092
F-score on testing data: 0.5998
</pre></div>


<h3>Effects of Feature Compression</h3>
<p>By reducing the number of predictor features we lose some accuracy (we go from 84.92% to 80.98%) and the F-score goes from .7 to .6. These differences are subtle so we'd need to understand the final use-case for the model to determine which one will better suite our needs.</p>
<p>By performing the reduction we'd expect our model training and prediction times to decrease. If the goal of our model is to produce the most accurate results we'd likely want to move forward with the full model, in contrast if the goal is to create an more parsimonious model we'd favor the smaller, more-lightweight model.</p>
<p><strong>Thanks for reading, please reach out with comments on <a href="http://twitter.com/etav3">twitter</a>! </strong></p>
</div>
    <aside>
    <div class="bug-reporting__panel">
        <h3>Find an error or bug? Have a suggestion?</h3>
        <p>Everything on this site is avaliable on GitHub. Head on over and <a href='https://github.com/etav/'>submit an issue.</a> You can also message me directly on <a href='https://twitter.com/etav'>Twitter</a>.</p>
    </div>
    </aside>
</section>

    </div>
    <!-- start of footer section -->
    <footer class="footer">
        <div class="container">
            <p class="text-muted">
                <center>This project contains 16 pages and is available on <a href="https://github.com/etav/">GitHub</a>.
                <br/>
                Copyright &copy; Ernest Tavares III,
                    <time datetime="2017">2017</time>.
                </center>
            </p>
        </div>
    </footer>

    <!-- This jQuery line finds any span that contains code highlighting classes and then selects the parent <pre> tag and adds a border. This is done as a workaround to visually distinguish the code inputs and outputs -->
    <script>
        $( ".hll, .n, .c, .err, .k, .o, .cm, .cp, .c1, .cs, .gd, .ge, .gr, .gh, .gi, .go, .gp, .gs, .gu, .gt, .kc, .kd, .kn, .kp, .kr, .kt, .m, .s, .na, .nb, .nc, .no, .nd, .ni, .ne, .nf, .nl, .nn, .nt, .nv, .ow, .w, .mf, .mh, .mi, .mo, .sb, .sc, .sd, .s2, .se, .sh, .si, .sx, .sr, .s1, .ss, .bp, .vc, .vg, .vi, .il" ).parent( "pre" ).css( "border", "1px solid #DEDEDE" );
    </script>

    <!-- Load Google Analytics -->
    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-66582-32', 'auto');
        ga('send', 'pageview');
    </script>
    <!-- End of Google Analytics -->

    <!-- Bootstrap core JavaScript
      ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../theme/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="../theme/js/ie10-viewport-bug-workaround.js"></script>
</body>

</html>