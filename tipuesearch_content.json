{"pages":[{"url":"https://etav.github.io/pages/about.html","text":"I am an aspiring data scientist with a passion for technology startups—I hope to one day found an AI/Machine Learning venture in Silicon Valley. Currently I work at SocialCode as an Advertising Manager but prior to that I was a student at UPenn where I became the first person in my family to earn a college degree (B.S. in Economics) with a triple concentration in Analytics, Marketing and Entrepreneurship from The Wharton School. I never considered myself a \"quant\" but during my time at Penn I challenged myself by taking as many statistics classes as my major would allow and fell in love with the subject. I also interned at 3 Philadelphia startups and DreamIt Ventures before co-founding a startup incubator, WeissLabs during my final year. My first exposure to coding was R but currently I'm learning Python. Email: hi@ernesttavares.co Twitter: @etav Resumé Education B.S., Economics , The Wharton School, University of Pennsylvania, Philadelphia, PA. 2016 Triple concentration in analytics, marketing and entrepreneurship, philosophy (minor) Experience Associate Advertising Manager , SocialCode , 2015 - Present Managing daily optimization and execution across all aspects of social advertising campaigns, ensuring client goals are met in a timely and efficient manner. Co-Founder , WeissLabs , 2015 - 2016 Responsible for all aspects of forming an student-run organization including: Initial team formation Sourcing student-run startups Developing a 7-week entrepreneurship curriculum Connecting startups with investors Business Analyst Intern , Leadnomics , 2014 - 2016 Leveraged operational data to create reports to inform business strategy. Inventoried machine learning software (such as: Google Tensorflow, vowpal wabbit, sci-kit learn) to evaluate accuracy and usability. Strategic Investment Intern , DreamIt Ventures , 2015 - 2015 Identified and contacted venture capitalist and angel investors who had a history of investing in early-stage healthcare startups.","tags":"pages","loc":"https://etav.github.io/pages/about.html","title":"About Ernest Tavares"},{"url":"https://etav.github.io/projects/random_forest_college_admissions.html","text":"College Admissions Exploratory Project in R 1. Introduction Matching high school students to colleges which will fit them well is a primary duties of high school guidance counselors. As the competition for jobs increases, attaining a bachelor's degree is a good way to differentiate and refine skills while providing time for students to mature into productive young adults. The main barrier to entry for many students is the hefty price tag of a college degree which has increased by 1,128% since 1978 (Bloomberg) . Currently the College Board reports the average private non-profit four-year tuition is $32,405 however, this price does factor in the cost of living for students which can greatly inflate this figure. The Wall Street Journal estimates that the average college graduate in 2015 will have $35,000 in student debt which is the highest level on record while the total dollar value attributable to student-loans in the US is rapidly approaching $70 billion . After considering both the potential future pay-off and the high-cost of pursuing a college education, the seriousness of this decision becomes obvious. Like many modern decision systems, the college application process relies heavily on data to make informed admissions decisions. As such, within the scope of this essay we will explore a data set from a specific college's admission office and use Random Forests from the CRAN() library to create a model which given inputs (ie: demographic information and scholastic metrics like GPA) will create a model to predict the likelihood of being admitted into a specific secondary institution. 2. The Data Our data set contains 8,700 observations and 9 variables. The data comes from a specific university's application office and each row contains variables related to the admission decision, the student's scholastic performance and other demographic information. In this section we will provide an overview of each variable before moving onto univariate analysis. Variable Name Description admit A binary factor where \"1\" indicates the student was admitted and \"0\" indicates a rejection. This is our response variable. anglo A binary factor where \"1\" indicates the student is Caucasian. asian A binary factor where \"1\" indicates the student is Asian. black A binary factor where \"1\" indicates the student is African American. gpa.weighted A numerical variable which provides a weighted GPA of the student (may be greater than 4.0 if the student was enrolled in AP courses). sati-verb A numerical variable capturing the student's verbal SAT score. sati-math A numerical variable capturing the student's math SAT score. income A numerical variable capturing the student's house-hold income, it has been capped at $100,000. sex A binary factor where \"1\" indicates the student is male. 2.1 Partitioning the Data: Training, Testing & Evaluation Sets For this implementation of the random forest algorithm we will not worry about creating training, testing and evaluation data sets because the randomForest function has a built-in OOB estimator which we can use to determine its performance and removing the necessity to set aside a training set. 2.2 Univariate Analysis In order to get a high-level understanding of our data we will examine the summary statistics conveniently provided by the R's summary() function before looking into any variable more specifically. Here's the output from the function: We will now comment on each variable independently. As we can tell from the output above, 2686 of the 8700 applicants were accepted which means admission rate for this university is: 2686/8700 = ~30.89% When comparing the admission rate for this university (31%) to that of the average 4-Year Private University (62.8%) we learn that it is twice as selective ( source ). Because all three of our our predictor variables related to a student's race capture the same type of information we will analyze them together. We have ethnic data for 6583 or approximately 76% of our test sample, we have 829 observations where there is missing ethnic data. Race Count Proportion (% of Total) Asian 3417 39% African American 356 4% Caucasian 2810 32% NA 856 10% Other 1261 15% Total 8700 100% Since the student's weighted GPA, SAT verbal and SAT Math scores are all numeric data-types, we will create histograms to analyze the distribution of each variable checking for normality and any potential outliers: Without performing sophisticated analysis we can tell these data look fairly normally distributed. However, we must note the large count of \"0s\" for SAT scores. Upon investigating it appears that we have missing data for 457 of our sample SAT scores. This should not cause too much concern because it is a relatively small proportion of our total sample. Furthermore, we should not remove the data because we can't tell if it is missing randomly or if there was systematic recording error present. We will keep these missing values in mind when we move on to model building. The final variable we will examine is house hold income. It is important to note that all house holds have been capped at an income of $100,000 , meaning that all income levels above this cut-off point will be labeled as \"$100,000\". This has a strong effect which becomes evident when we create a histogram of for this variable. ## 3. Multivariate Analysis We will limit the exploration of multivariate graphs and statistical relationships to numerical data, or specifically: SAT Math and SAT verbal scores, weighted GPA and household income . However, since Math SAT scores and Verbal SAT scores are strongly correlated (R-squared =.736), and thus explain similiar variance within the dataset we will only consider Math scores. Below are summary plots to visualize the relationships between our numerical variables. ## 4. Random Forests Background 4.1 Technicalities R's CART (Classification And Regression Trees) package which we will use to call the RandomForest function uses the following equation to define a splitting decision: ΔI(s,A) = I(A) - p(A_L)I(A_L) - p(A_R)I(A_R) Where I(A) is the value of \"impurity\" in a parent node, p(A_L) is the likelihood of a given case falling to the left of the parent node and I(A_L) is the impurity of the daughter node resulting from the split, while P(A_R) and I(A_R) capture similar information but for the right daughter node. Given the above formula the CART algorithm selects a predictor which maximizes ΔI(s,A). By doing so the algorithm is creating terminal nodes which look as homogeneous as possible within the node but as heterogeneous as possible when compared to other terminal nodes . By now it should become clear that Random Forests are simply a large number of decision trees, each node of which is subject to the above optimization ( CART Docs ). In reality, the formula for creating a splitting decision is the same for CART as RandomForest , the key difference is that RandomForest creates numerous decision trees sacrificing interpretability for increased predictive power. 5. Model Building With Random Forests After performing our variable exploration, univariate and bivarate analysis I feel comfortable moving on to building a Random Forest classifier using our data set. Without any model tuning, the algorithm provides the following confusion matrix: State Predicted: Not Admitted Predicted: Admitted Model Error Not Admitted (\"0\") 4240 308 6.7% Admitted (\"1\") 716 1240 36.6% Use Error 14.5% 19.9% Overall Error = 15.7% We have created a collection or \"forest\" featuring 500 decision trees. In this scenario we have not tuned our parameters and we will accept the default values for priors, this will serve as our baseline model which we can improve upon by tuning priors. The Out of Bag estimate for error rate provided is 15.7%. We also note that our model is under predicting the number of students which should be classified as being admitted (our model predicts 23.8% of students will be admitted but we know from our univariate analysis in section 2.2 that the admit rate should be closer to 31%). 5.1 Calling Random Forests #-----MODEL BUILDING: RANDOM FORESTS CALL----- library ( randomForest ) # random forests rf1 <- randomForest ( admit ~ . , data = data , importance = T , na.action = na.fail , ntree = 500 ) rf1 5.2 Model Tuning To correct for the algorithm under predicting the total number of students admitted we introduce priors and assign a cost ratio of 3 to 1 . What this means is that we are giving the algorithm \"slack\" while predicting whether a student is admitted and tightening our threshold for predicting whether someone is not admitted. After this modification we would expect a false positive rate 3 times greater than a false negative rate . The reason for selecting this ratio is because it seems more costly to fail to predict when someone is admitted compared to the scenario in which a student is classified as being admitted but was actually not.After running our entire data set through the randomForest() algorithm and setting our cost ration equal to 3 to 1, we arrive at the confusion matrix below: State Predicted: Not Admitted Predicted: Admitted Model Error Not Admitted (\"0\") 3536 1012 22.2% Admitted (\"1\") 294 1662 15% Use Error 7.7% 37.9% Overall Error = 20% Notice how the forecasted total number of students admitted went up, increasing the use error but decreasing the model error. 5.3 Model Evaluation As we see from the plot below the most important variables for predicting whether a student is admitted to this university are those related to academic achievement. For example, Weighted GPA, SAT Math and SAT Verbal Scores explain the most variance within the dataset for our model . While factors like income and race contributed to the accuracy, their effect was less pronounced in our model. We created partial dependency plots for each of our quantitative variables. The plots for weighted GPA begins to increase at 2.0 and peak around 3.5 meaning that values of weighted GPA between 3 and 3.5 tend to predict \"Admitted = True\" strongly. For both SAT verbal and SAT Math scores we see a similar trend. The odds of being admitted increase as levels of income reach above $90,000, see the partial dependency plots below for more details: 5.4 Conclusion Through this analysis we have gained a deeper understanding into the data behind college admissions. By analyzing the variables available to us we were able to determine that variables related to scholastic performance are much better for predicting the admittance rate of an individual student when compared with socioeconomic and racial factors . We tuned our model to a 3 to 1 cost ratio for false positives and false negatives, meaning we penalized the model by a factor of 3 to 1 when predicting whether a student was accepted. Initially the baseline model produced too many false negatives which was something I had not anticipated. For future analysis I would like to have a larger dataset which includes more than 1 university.","tags":"Projects","loc":"https://etav.github.io/projects/random_forest_college_admissions.html","title":"College Admissions Exploratory Analysis in R"},{"url":"https://etav.github.io/python/scikit_pca.html","text":"Principal Component Analysis (PCA) in Python using Scikit-Learn Principal component analysis is a technique used to reduce the dimensionality of a data set. PCA is typically employed prior to implementing a machine learning algorithm because it minimizes the number of variables used to explain the maximum amount of variance for a given data set. PCA Introduction PCA uses \"orthogonal linear transformation\" to project the features of a data set onto a new coordinate system where the feature which explains the most variance is positioned at the first coordinate (thus becoming the first principal component). Source PCA allows us to quantify the trade-offs between the number of features we utilize and the total variance explained by the data. PCA allows us to determine which features capture similiar information and discard them to create a more parsimonious model. In order to perform PCA we need to do the following: PCA Steps Standardize the data. Use the standardized data to create a covariance matrix. Use the resulting matrix to calculate eigenvectors (principal components) and their corresponding eigenvalues. Sort the components in decending order by its eigenvalue. Choose n components which explain the most variance within the data (larger eigenvalue means the feature explains more variance). Create a new matrix using the n components. NOTE: PCA compresses the feature space so you will not be able to tell which variables explain the most variance because they have been transformed. If you'd like to preserve the original features to determine which ones explain the most variance for a given data set, see the SciKit Learn Feature Documentation . Resources 1. District data labs 2. chris 3. implementation 4. More #Imports import matplotlib.pyplot as plt import numpy as np import pandas as pd from sklearn import decomposition from sklearn.preprocessing import scale from sklearn.decomposition import PCA import seaborn as sb % matplotlib inline /Users/ernestt/venv/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment. warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.') sb . set ( font_scale = 1.2 , style = \"whitegrid\" ) #set styling preferences loan = pd . read_csv ( 'loan.csv' ) . sample ( frac = . 25 ) #read the dataset and sample 25% of it / Users / ernestt / venv / lib / python2 . 7 / site - packages / IPython / core / interactiveshell . py : 2717 : DtypeWarning : Columns ( 19 , 55 ) have mixed types . Specify dtype option on import or set low_memory = False . interactivity = interactivity , compiler = compiler , result = result ) For this example, we're going to use the Lending Club data set which can be found here . #Data Wrangling loan . replace ([ np . inf , - np . inf ], np . nan ) #convert infs to nans loan = loan . dropna ( axis = 1 , how = 'any' ) #remove nans loan = loan . _get_numeric_data () #keep only numeric features Step 1: Standardize the Dataset x = loan . values #convert the data into a numpy array x = scale ( x ); x array([[ 1.17990021, 1.17491004, -0.61220612, ..., -0.07607754, -0.38999916, 0. ], [ 1.57614469, 1.58965176, 0.14553604, ..., -0.07607754, -0.45317429, 0. ], [ 0.50760835, 0.50047945, 0.40304998, ..., -0.07607754, -0.35598935, 0. ], ..., [ 1.16244466, 1.15544092, 0.85591931, ..., -0.07607754, -0.34906088, 0. ], [-1.13519249, -1.11536499, -0.6299657 , ..., -0.07607754, 0.6887011 , 0. ], [ 1.35264446, 1.35535277, 0.26393325, ..., -0.07607754, 3.15726473, 0. ]]) Step 2: Create a Covariance Matrix covar_matrix = PCA ( n_components = 20 ) #we have 20 features Step 3: Calculate Eigenvalues covar_matrix . fit ( x ) variance = covar_matrix . explained_variance_ratio_ #calculate variance ratios var = np . cumsum ( np . round ( covar_matrix . explained_variance_ratio_ , decimals = 3 ) * 100 ) var #cumulative sum of variance explained with [n] features array([ 33. , 58.9, 68.8, 75.8, 81.6, 86.7, 91.8, 95.3, 97.2, 98.4, 99.4, 99.8, 100.1, 100.1, 100.1, 100.1, 100.1, 100.1, 100.1, 100.1]) In the above array we see that the first feature explains roughly 33% of the variance within our data set while the first two explain 58.9 and so on. If we employ 10 features we capture 98.4% of the variance within the dataset, thus we gain very little by implementing an additional feature (think of this as diminishing marginal return on total variance explained). Step 4, 5 & 6: Sort & Select plt . ylabel ( '% Variance Explained' ) plt . xlabel ( '# of Features' ) plt . title ( 'PCA Analysis' ) plt . ylim ( 30 , 100.5 ) plt . style . context ( 'seaborn-whitegrid' ) plt . plot ( var ) [<matplotlib.lines.Line2D at 0x11666a910>] Based on the plot above it's clear we should pick 10 features.","tags":"Python","loc":"https://etav.github.io/python/scikit_pca.html","title":"Principle Component Analysis (PCA) with Scikit-Learn"},{"url":"https://etav.github.io/python/scatter_plot_python_seaborn.html","text":"Scatter Plot using Seaborn One of the handiest visualization tools for making quick inferences about relationships between variables is the scatter plot. We're going to be using Seaborn and the boston housing data set from the Sci-Kit Learn library to accomplish this. import pandas as pd import seaborn as sb % matplotlib inline from sklearn import datasets import matplotlib.pyplot as plt sb . set ( font_scale = 1.2 , style = \"ticks\" ) #set styling preferences dataset = datasets . load_boston () #convert to pandas data frame df = pd . DataFrame ( dataset . data , columns = dataset . feature_names ) df [ 'target' ] = dataset . target df . head () df = df . rename ( columns = { 'target' : 'median_value' , 'oldName2' : 'newName2' }) df . DIS = df . DIS . round ( 0 ) Describe the data df . describe () . round ( 1 ) CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT median_value count 506.0 506.0 506.0 506.0 506.0 506.0 506.0 506.0 506.0 506.0 506.0 506.0 506.0 506.0 mean 3.6 11.4 11.1 0.1 0.6 6.3 68.6 3.8 9.5 408.2 18.5 356.7 12.7 22.5 std 8.6 23.3 6.9 0.3 0.1 0.7 28.1 2.1 8.7 168.5 2.2 91.3 7.1 9.2 min 0.0 0.0 0.5 0.0 0.4 3.6 2.9 1.0 1.0 187.0 12.6 0.3 1.7 5.0 25% 0.1 0.0 5.2 0.0 0.4 5.9 45.0 2.0 4.0 279.0 17.4 375.4 7.0 17.0 50% 0.3 0.0 9.7 0.0 0.5 6.2 77.5 3.0 5.0 330.0 19.0 391.4 11.4 21.2 75% 3.6 12.5 18.1 0.0 0.6 6.6 94.1 5.0 24.0 666.0 20.2 396.2 17.0 25.0 max 89.0 100.0 27.7 1.0 0.9 8.8 100.0 12.0 24.0 711.0 22.0 396.9 38.0 50.0 Variable Key Variable Name CRIM per capita crime rate by town ZN proportion of residential land zoned for lots over 25,000 sq.ft. INDUS proportion of non-retail business acres per town CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) NOX nitric oxides concentration (parts per 10 million) RM average number of rooms per dwelling AGE proportion of owner-occupied units built prior to 1940 DIS weighted distances to five Boston employment centres RAD index of accessibility to radial highways TAX full-value property-tax rate per \\$10,000 PTRATIO pupil-teacher ratio by town B 1000(Bk - 0.63)&#94;2 where Bk is the proportion of blacks by town LSTAT % lower status of the population median_value Median value of owner-occupied homes in $1000's via UCI Barebones scatter plot plot = sb . lmplot ( x = \"RM\" , y = \"median_value\" , data = df ) Add some color and re-label points = plt . scatter ( df [ \"RM\" ], df [ \"median_value\" ], c = df [ \"median_value\" ], s = 20 , cmap = \"Spectral\" ) #set style options #add a color bar plt . colorbar ( points ) #set limits plt . xlim ( 3 , 9 ) plt . ylim ( 0 , 50 ) #build the plot plot = sb . regplot ( \"RM\" , \"median_value\" , data = df , scatter = False , color = \".1\" ) plot = plot . set ( ylabel = 'Median Home Price ($1000s)' , xlabel = 'Mean Number of Rooms' ) #add labels","tags":"Python","loc":"https://etav.github.io/python/scatter_plot_python_seaborn.html","title":"Scatter Plot in Python using Seaborn"},{"url":"https://etav.github.io/python/pairs_plot_python_seaborn.html","text":"Creating a Pairs Plot using Python One of my favorite functions in R is the pairs plot which makes high-level scatter plots to capture relationships between multiple variables within a dataframe. To my knowledge, python does not have any built-in functions which accomplish this so I turned to Seaborn , the statistical visualization library built on matplotlib, to accomplish this. #import seaborn import seaborn as sb sb . set ( font_scale = 1.35 , style = \"ticks\" ) #set styling preferences % matplotlib inline Load the example iris dataset iris = sb . load_dataset ( \"iris\" ) iris . head () sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa Look at a summary of the data iris . describe () sepal_length sepal_width petal_length petal_width count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.057333 3.758000 1.199333 std 0.828066 0.435866 1.765298 0.762238 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 plot = sb . pairplot ( iris , hue = 'species' ) Change the bar plot to lines graphs. plot = sb . pairplot ( iris , hue = 'species' , diag_kind = 'kde' ) Change the palette of the plot. plot = sb . pairplot ( iris , hue = 'species' , diag_kind = 'kde' , palette = 'husl' )","tags":"Python","loc":"https://etav.github.io/python/pairs_plot_python_seaborn.html","title":"Pairs Plot in Python using Seaborn"},{"url":"https://etav.github.io/algorithms/knn_implementation_scikit-learn.html","text":"K-Nearest Neighbors Resources: Wikipedia SciKit-Learn StatSoft Definition K-Nearest Neighbors Algorithm (aka kNN) can be used for both classification (data with discrete variables) and regression (data with continuous labels). The algorithm functions by calculating the distance (Sci-Kit Learn uses the formula for Euclidean distance but other formulas are available) between instances to create local \"neighborhoods\". K-Nearest Neighbors functions by maximizing the homogeneity amongst instances within a neighborhood while also maximizing the heterogeneity of instances between neighborhoods. So each member of a given neighborhood \"looks like\" (has similar variables) all of the other members of that neighborhood. One neat feature of the K-Nearest Neighbors algorithm is the number of neighborhoods can be user defined or generated by the algorithm using the local density of points. The Scikit—Learn Function: sklearn.neighbors accepts numpy arrays or scipy.sprace matrices are inputs. For this implementation I will use the classic 'iris data set' included within scikit-learn as a toy data set. Overview of the Data % matplotlib inline import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sb from sklearn.linear_model import LinearRegression from scipy import stats import pylab as pl /Users/ernestt/venv/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment. warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.') from sklearn.datasets import load_iris data = load_iris () print 'Keys:' , data . keys () print '-' * 20 print 'Data Shape:' , data . data . shape print '-' * 20 print 'Features:' , data . feature_names print '-' * 20 Keys: ['target_names', 'data', 'target', 'DESCR', 'feature_names'] -------------------- Data Shape: (150, 4) -------------------- Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] -------------------- What the hell is a Sepal? Personally, I had no idea what a sepal was so I looked up some basic flower anatonmy, I found this picture helpful for relating petal and sepal length. Flower Anatomy: Scatter Plots #Petal Length vs Sepal Width plt . scatter ( data . data [:, 1 ], data . data [:, 2 ], c = data . target , cmap = plt . cm . get_cmap ( 'Set1' , 3 )) plt . xlabel ( data . feature_names [ 1 ]) plt . ylabel ( data . feature_names [ 2 ]) color_bar_formating = plt . FuncFormatter ( lambda i , * args : data . target_names [ int ( i )]) plt . colorbar ( ticks = [ 0 , 1 , 2 ], format = color_bar_formating ) <matplotlib.colorbar.Colorbar at 0x10d23fa50> #Petal Length vs Sepal Width plt . scatter ( data . data [:, 2 ], data . data [:, 3 ], c = data . target , cmap = plt . cm . get_cmap ( 'Set1' , 3 )) plt . xlabel ( data . feature_names [ 2 ]) plt . ylabel ( data . feature_names [ 3 ]) color_bar_formating = plt . FuncFormatter ( lambda i , * args : data . target_names [ int ( i )]) plt . colorbar ( ticks = [ 0 , 1 , 2 ], format = color_bar_formating ) <matplotlib.colorbar.Colorbar at 0x10d505fd0> This plot indicates a strong positive correlation between petal length and width for each of the three flower species import pandas as pd iris_data = pd . read_csv ( 'iris-data.csv' ) iris_data . loc [ iris_data [ 'class' ] == 'versicolor' , 'class' ] = 'Iris-versicolor' #clean species labels iris_data . loc [ iris_data [ 'class' ] == 'Iris-setossa' , 'class' ] = 'Iris-setosa' sb . pairplot ( iris_data . dropna (), hue = 'class' ) #too many species <seaborn.axisgrid.PairGrid at 0x1115a3250> Implementing K-Nearest Neighbors Classifier In this example we're using kNN as a classifier to identify what species a given flower most likely belongs to, given the following four features (measured in cm): sepal length sepal width petal length petal width Essentially this is what is happening under the hood: 1. We use our data to train The kNN Classifier. * This will allow the algorithm to define new neighborhoods. 2. Once the neighborhoods are defined, our classifier will be able to ingest feature data (petal and sepal measurements) on flowers it has not been trained on and determine which neighborhood it is most homogenous to. * Once the neighborhoods have been defined we can actually use the classifier in a generalizable fashion on new data. 3. We can determine the accuracy (and usefulness) of our model by seeing how many flowers it accurately classifies on a testing data set. * In order to do this the actual species must be known. from sklearn import neighbors , datasets # where X = measurements and y = species X , y = data . data , data . target #define the model knn = neighbors . KNeighborsClassifier ( n_neighbors = 5 , weights = 'uniform' ) #fit/train the new model knn . fit ( X , y ) #What species has a 2cm x 2cm sepal and a 4cm x 2cm petal? X_pred = [ 2 , 2 , 4 , 2 ] output = knn . predict ([ X_pred ,]) #use the model we just created to predict print 'Predicted Species:' , data . target_names [ output ] print 'Options:' , data . target_names print 'Probabilities:' , knn . predict_proba ([ X_pred , ]) Predicted Species: ['versicolor'] Options: ['setosa' 'versicolor' 'virginica'] Probabilities: [[ 0. 0.8 0.2]] kNN Decision Boundary Plot Here's a graphical representation of the classifier we created above. As we can see from this plot, the virgincia species is relatively easier to classify when compared to versicolor and setosa. kNN Plot Conclusion The number of neighbors to implement is highly data-dependent meaning optimal neighborhood sizes will differ greatly between data sets. It is important to select a classifier which balances generalizability (precision) and accuracy or we are at risk of overfitting. For example, if we pick a classifier which fits the data perfectly we will lose the ability to make generalizable inferences from it (this would look like the 'low accuracy', 'high precision' scenario below because our model is very good at predicting training data but misses completely when presented with new data). Best practice is to test multiple classifiers using a testing data set to ensure we're making appropriate trade-offs between accuracy and generalizability. We're shooting for high-accuracy and high-precision","tags":"Algorithms","loc":"https://etav.github.io/algorithms/knn_implementation_scikit-learn.html","title":"K-Nearest Neighbors Implementation using Scikit-Learn"},{"url":"https://etav.github.io/python/importing_csv_into_pandas.html","text":"Import necessary modules import pandas as pd import numpy as np Create a toy dataframe (to be converted into csv) data = { 'name' :[ 'Ernest' , 'Jason' , 'Kevin' , 'Christine' ], 'job' :[ 'Analyst' , 'Nerd' , 'Teacher' , 'Product Manager' ], 'salary' :[ '$60,000' , '' , '$70,000' , '$80,000' ]} df = pd . DataFrame ( data , columns = [ 'name' , 'job' , 'salary' ]) df name job salary 0 Ernest Analyst $60,000 1 Jason Nerd 2 Kevin Teacher $70,000 3 Christine Product Manager $80,000 Export the dataframe to a csv in the current directory df . to_csv ( 'career_info.csv' ) Now, load the csv df = pd . read_csv ( 'career_info.csv' ) df Unnamed: 0 name job salary 0 0 Ernest Analyst $60,000 1 1 Jason Nerd NaN 2 2 Kevin Teacher $70,000 3 3 Christine Product Manager $80,000 Notice Pandas conveniently pulls in the header information without needing specification","tags":"Python","loc":"https://etav.github.io/python/importing_csv_into_pandas.html","title":"Importing a CSV Into Pandas"},{"url":"https://etav.github.io/articles/machine_learning_supervision_optional.html","text":"Machine learning is defined as a subfield of computer science and artificial intelligence which \"gives computers the ability to learn without being explicitly programmed\" (source) . Although the statistical techniques which underpin machine learning have existed for decades recent developments in technology such as the availability/affordability of cloud computing and the ability to store and manipulate big data have accelerated its adoption. This essay is meant to explore the most popular methods currently being employed by data scientists such as supervised and unsupervised methods to people with little to no understanding of the field. (An example of a support vector machine (SVM) algorithm being used to create a decision boundary (via wikipedia ) Supervised Supervised machine learning describes an instance where inputs along with the outputs are known. We know the beginning and the end of the story and the challenge is to find a function (story teller, if you will) which best approximates the output in a generalizable fashion. Example : Imagine a doctor trying to predict whether someone has HIV. He has the test results (outputs) and medical records (variables) for patients who have tested positive and negative for the disease. His task is to look at the records and develop a decisioning system so that when a new patient arrives, given just their medical record (variables) he can accurately predict whether or not they are HIV positive. (A graphic represention a logistic regression) Unsupervised Unsupervised machine learning is a bit more abstract because it describes a scenario in which only know the input variables are known but nothing about the outputs are known. A typical task for this type of machine learning is clustering , or grouping the data by using features, to arrive at generalizable insights. Clustering using K-Nearest Neighbors algorithm Semi-Supervised Semi-supervised learning is a hybrid of supervised and unsupervised machine learning . This describes a scenario where there is a small portion of labeled data mixed with unlabeled data. Acquiring labeled data is costly because typically it requires manual input from humans to generate. Semi-supervised learning allows for quicker turn-around while sacrificing accuracy (increasing labeled data increases model accuracy) but it is usually more accurate than unsupervised learning alone. Reinforcement learning Reinforcement learning , or simulation based optimization is a relatively lesser known branch of machine learning but where the future of AI is headed because it requires almost no human input . It's also what enabled Google's Alpha Go to be so successful—and which we'll use for illustrative purposes (here's the research paper ) . Reinforcement learning describes a situation in which humans provide the following: An environment (A Go board) A set of rules (The rules of Go) A Set of actions (All the actions that can be taken) A set of outcomes (Win or lose) Then given an environment, rules, actions and outcomes a computer can repeatedly simulate many, many games of Go and \"learn\" (optimzie) for what strategies work best in a given scenario. What makes reinforcement learning extremely powerful is the sheer number of times a computer can simulate unique games. By the time Alpha Go faced Lee Sedol, the top player in the world, it had simulated more games than Sedol could've ever hoped to play in his lifetime. Humans need to eat, sleep and take breaks, computers don't they just require electricity. There you have it, a quick and dirty overview of some of the more popular machine learning methods currently being employed. Follow me on medium to show your support!","tags":"Articles","loc":"https://etav.github.io/articles/machine_learning_supervision_optional.html","title":"Machine Learning Supervision Optional"},{"url":"https://etav.github.io/projects/random_forest_animal_shelter.html","text":"Animal Shelter The Dataset Some R code for a kaggle competition I entered. The goal was to create a classifier to predict the outcome of a sheltered animal using features such the animal's gender, age and breed. The training dataset contains 26,729 observations, 9 predictor variables and was given to us by the Austin Animal Shelter. Exploratory Analysis Before I dive into creating a classifier, I typically perform an exploratory analysis moving from observing one univariate statistics to bivariate statistics and finally model building. However, I broke from my normal process as curiosity got the best of me. I was interested in learning about what the typical outcomes are for sheltered animals (check out the graph below). Luckily, as we see above, many animals are either adopted, transferred or in the case of dogs frequently returned to their owners. The Variables [ 1 ] \"ID\" \"Name\" \"DateTime\" \"OutcomeType\" \"OutcomeSubtype\" \"AnimalType\" [ 7 ] \"SexuponOutcome\" \"AgeuponOutcome\" \"Breed\" \"Color\" Variable Name Description ID The animal's unique ID. Name The animal's name, if known (many are not). DateTime The date and time the animal entered the shelter (ranges from 1/1/14 - 9/9/15). OutcomeType A five factor variable detailing the outcome for the animal (ie: adopted,transferred, died). OutcomeSubtype 17 factor variable containing Further details related to the outcome of the animal, such as whether or not they were aggressive. AnimalType Whether the animal is a cat or dog. SexuponOutcome The sex of the animal at the time the outcome was recorded. AgeuponOutcome The age of the animal when the outcome was recorded. Breed The breed of the animal (contains mixed breed). Color A Description of the coloring on the animal. Transforming Variables The first thing I did was transform the date variable by separating time and date so that I can analyze them independently, I'd like to be able to compare time of day and any seasonality effects on adoption. I then moved on to address missing name values (there were a few mis-codings which caused errors). After that I moved onto transforming the \"AgeuponOutcome\" variable so that the reported age of animals would all be in the same units, I chose days. This took some chaining of ifelse statements: Animal's Age #Animal Age split <- str_split_fixed ( train $ AgeuponOutcome , \" \" , 2 ) # split value and unit of time split [, 2 ] <- gsub ( \"s\" , \"\" , split [, 2 ]) #remove tailing \"s\" #create a vector to multiply multiplier <- ifelse ( split [, 2 ] == 'day' , 1 , ifelse ( split [, 2 ] == 'week' , 7 , ifelse ( split [, 2 ] == 'month' , 30 , ifelse ( split [, 2 ] == 'year' , 365 , NA )))) train $ days_old <- as.numeric ( split [, 1 ]) * multiplier #apply the multiplier train $ days_old [ 1 : 5 ] #compare, looks good train $ AgeuponOutcome [ 1 : 5 ] After this transformation, we're able to create a visualization which tells us the outcome of each animal type as a function of its age (in days). Interestingly the likelihood of being adopted for cats varies with age whereas for dogs there appears to be a slight negative correlation between a it's age and the probability it will be adopted. For dogs, it seems older animals tend to have a higher likelihood of being returned to their owner (I assume this is has to do with the proliferation of chips for animals) Animal's Gender Moving on I decided to compare the differences in outcomes based on the animal's gender. It's clear that adopters favor animals (both cats and dogs) that have previously been neutered. It's interesting to note that a large proportion of cats which were not neutered are transferred to another animal shelter, where (my guess is) they are then neutered. Applying Random Forest After transforming our variables, performing univariate analysis and determining the validity of our sample, it's finally time to move to model building. I will create a random forest using the RandomForest package, using OutcomeType as our predictor variable (remember there are five levels, which complicates things a bit). rf1 <- randomForest ( OutcomeType ~ AnimalType + SexuponOutcome + Named + days_old + young + color_simple , data = train , importance = T , ntree = 500 , na.action = na.fail ) rf1 Our random forest model does poorly at classifying animal deaths which makes sense when we consider only 197/26729 or 0.007370272% of our training set were flagged as \"Died\". The model does fairly well at predicting instances where an adoption, transfer, or euthanasia occurs which make up the bulk of the training set. Furthermore, our OOB or out of bag error estimate is 35.28%. Here's a detailed breakdown of our Random Forest: #Calling Random Forest Call : randomForest ( formula = OutcomeType ~ AnimalType + SexuponOutcome + Named + days_old + young + color_simple , data = train , importance = T , ntree = 500 , na.action = na.fail ) Type of random forest : classification Number of trees : 500 No. of variables tried at each split : 2 OOB estimate of error rate : 35.28 % Confusion matrix : Adoption Died Euthanasia Return_to_owner Transfer class.error Adoption 8988 0 2 1391 388 0.1653821 Died 21 0 10 10 156 1.0000000 Euthanasia 229 0 175 380 771 0.8874598 Return_to_owner 1998 0 9 2393 386 0.5000000 Transfer 2641 0 83 954 5744 0.3903630 Determining Variable Importance Finally, we'll rank our predictor variables based on their mean reduction in Gini error. As we see in the graphic above, the animal's sex and age were most useful to reduce the mean Gini Error. Interestingly, animal type (ie: cat or dog) and the physical features of the animal such as the color mattered less. That's all for this one folks, thanks for tuning in!","tags":"Projects","loc":"https://etav.github.io/projects/random_forest_animal_shelter.html","title":"Animal Shelter Classifier using Random Forest"}]}