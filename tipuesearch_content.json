{"pages":[{"url":"https://etav.github.io/pages/about.html","text":"I am an aspiring data scientist with a passion for technology startups—I hope to one day found an AI/Machine Learning venture in Silicon Valley. Currently I work at SocialCode as an Advertising Manager but prior to that I was a student at UPenn where I became the first person in my family to earn a college degree (B.S. in Economics) with a triple concentration in Analytics, Marketing and Entrepreneurship from The Wharton School. I never considered myself a \"quant\" but during my time at Penn I challenged myself by taking as many statistics classes as my major would allow and fell in love with the subject. I also interned at 3 Philadelphia startups and DreamIt Ventures before co-founding a startup incubator, WeissLabs during my final year. My first exposure to coding was R but currently I'm learning Python. Email: hi@ernesttavares.co Twitter: @etav Resumé Education B.S., Economics , The Wharton School, University of Pennsylvania, Philadelphia, PA. 2016 Triple concentration in analytics, marketing and entrepreneurship, philosophy (minor) Experience Associate Advertising Manager , SocialCode , 2015 - Present Managing daily optimization and execution across all aspects of social advertising campaigns, ensuring client goals are met in a timely and efficient manner. Co-Founder , WeissLabs , 2015 - 2016 Responsible for all aspects of forming an student-run organization including: Initial team formation Sourcing student-run startups Developing a 7-week entrepreneurship curriculum Connecting startups with investors Business Analyst Intern , Leadnomics , 2014 - 2016 Leveraged operational data to create reports to inform business strategy. Inventoried machine learning software (such as: Google Tensorflow, vowpal wabbit, sci-kit learn) to evaluate accuracy and usability. Strategic Investment Intern , DreamIt Ventures , 2015 - 2015 Identified and contacted venture capitalist and angel investors who had a history of investing in early-stage healthcare startups.","tags":"pages","loc":"https://etav.github.io/pages/about.html","title":"About Ernest Tavares"},{"url":"https://etav.github.io/projects/classification_pipeline.html","text":"Classifying Charity Donors In this project, I will employed several supervised algorithms to accurately model individuals' income using data collected from the 1994 U.S. Census. I then identified the best candidate algorithm from preliminary results and further optimized this algorithm to best model the data. The goal of this analysis is to construct a model that accurately predicts whether an individual makes more than $50,000 USD annually. This sort of task can is common in a non-profit settings, where organizations survive on donations. Armed with information on an individual's income level aids non-profits in understanding how large of a donation to request, or whether they should contact certain individuals begin with. While it can be difficult to determine an individual's general income, we can (as we will see) infer this value from other publicly available features such as government census studies. The dataset for this project originates from the UCI Machine Learning Repository . The datset was donated by Ron Kohavi and Barry Becker, after being published in the article \"Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid\" . You can find the article by Ron Kohavi online . The data we investigate here consists of small changes to the original dataset, such as removing the 'fnlwgt' feature and records with missing or ill-formatted entries. I completed this project as part of the Udacity Machine Learning Engineer Nano Degree # Import libraries necessary for this project import numpy as np import pandas as pd from time import time from IPython.display import display # Allows the use of display() for DataFrames from __future__ import division #for division # Import supplementary visualization code import visuals as vs # Pretty display for notebooks % matplotlib inline # Load the Census dataset data = pd . read_csv ( \"census.csv\" ) # Success - Display the first record display ( data . head ( 1 )) age workclass education_level education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country income 0 39 State-gov Bachelors 13 Never-married Adm-clerical Not-in-family White Male 2174 0 40 United-States <=50K #summary stats data . describe () . astype ( int ) age education-num capital-gain capital-loss hours-per-week count 45222 45222 45222 45222 45222 mean 38 10 1101 88 40 std 13 2 7506 404 12 min 17 1 0 0 1 25% 28 9 0 0 40 50% 37 10 0 0 40 75% 47 13 0 0 45 max 90 16 99999 4356 99 Data Exploration As a preliminary step let's determine how many individuals within our data set fall into each of our two buckets: - Individuals making more than \\$50K annually - Individuals making less than \\$50K annually Also bear in mind that this census data was from 1994 and annual wages have inflated since then. # Total number of records n_records = data . shape [ 0 ] # Number of records where individual's income is more than $50,000 n_greater_50k = data . income . value_counts ()[ 1 ] . astype ( int ) # Number of records where individual's income is at most $50,000 n_at_most_50k = data . income . value_counts ()[ 0 ] . astype ( int ) # Percentage of individuals whose income is more than $50,000 greater_percent = n_greater_50k / n_records # Print the results print \"Total number of records: {}\" . format ( n_records ) print \"Individuals making more than $50,000: {}\" . format ( n_greater_50k ) print \"Individuals making at most $50,000: {}\" . format ( n_at_most_50k ) print \"Percentage of individuals making more than $50,000: {:.2f}%\" . format ( greater_percent * 100 ) Total number of records: 45222 Individuals making more than $50,000: 11208 Individuals making at most $50,000: 34014 Percentage of individuals making more than $50,000: 24.78% Data Pre-Processing Before we move on to modeling we're going to perform some preprocessing on our dataset to adjust the quality of our variables. For example, since we're dealing with a monetary response variable income , it's common to perform log transformations to normalize its distribution. # Split the data into features and target label income_raw = data [ 'income' ] features_raw = data . drop ( 'income' , axis = 1 ) # Visualize skewed continuous features of original data vs . distribution ( data ) Log Transformation Notice the strong positive skew present in the capital-gain and capital-loss features. In order to compress the range of our dataset and deal with outliers we will perform a log transformation using np.log() . However, it's important to remember that the log of zero is undefined so we will add 1 to each sample. # Log-transform the skewed features skewed = [ 'capital-gain' , 'capital-loss' ] features_raw [ skewed ] = data [ skewed ] . apply ( lambda x : np . log ( x + 1 )) #add 1 # Visualize the new log distributions vs . distribution ( features_raw , transformed = True ) The new distribution, which is still non-normally distributed, is much better than our initial state. This effect is more pronounced on the capital-gain feature . Scaling (Normalizing) Numeric Features After implementing our log transformation, it's good practice to perform scaling on numerical features so that each feature will be weighted equally when we have our algorithm ingest it. NOTE: once scaling has been applied, the features will not be recognizable. To do this we'll employ sklearn's sklearn.preprocessing.MinMaxScaler . Any outliers will dramatically effect the results of the scaling, that's why we handled them with a log transformation in the previous step. What's happening under-the-hood of this function is a simple division and subtraction to re-weight each sample within each feature such that they all fall within the range (0,1). The math behind the function is displayed below: %% latex $$ \\ x_ { scaled } = \\ frac { X - X_ { min }}{ X_ { max } - X_ { min }} $$ $$\\ x_{scaled}=\\frac{X - X_{min}}{X_{max} - X_{min}} $$ from sklearn.preprocessing import MinMaxScaler # Initialize a scaler, then apply it to the features scaler = MinMaxScaler () numerical = [ 'age' , 'education-num' , 'capital-gain' , 'capital-loss' , 'hours-per-week' ] features_raw [ numerical ] = scaler . fit_transform ( data [ numerical ]) # Show an example of a record with scaling applied display ( features_raw . head ( 1 )) age workclass education_level education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country 0 0.30137 State-gov Bachelors 0.8 Never-married Adm-clerical Not-in-family White Male 0.02174 0 0.397959 United-States Encoding Categorical Features Now that the numeric features have been transformed and scaled, it's time to give our categorical features some love. Since most machine learning algorithms will expect all features to be numeric, we'll perform feature encoding using pandas pd.get_dummies() function which will transform category values into numeric dummy variables within a dataframe. We'll also encode the the response variable (income) with 0 = income less than \\$50K and 1 = income greater than \\$50K. # Encode categorical features features = pd . get_dummies ( features_raw ) # Encode the 'income_raw' data to numerical values income = income_raw . replace ({ '<=50K' : 0 , '>50K' : 1 }) # Print the number of features after one-hot encoding encoded = list ( features . columns ) print \"{} total features after encoding.\" . format ( len ( encoded )) 103 total features after encoding. Shuffle Split the Data Now that we've transformed and scaled our numeric features and encoded both our categorical features as well as our response variable we're ready to split our data set into training and testing sets. # Import train_test_split from sklearn.cross_validation import train_test_split # Split the 'features' and 'income' data into training and testing sets X_train , X_test , y_train , y_test = train_test_split ( features , income , test_size = 0.2 , random_state = 0 ) # Show the results of the split print \"Training set has {} samples.\" . format ( X_train . shape [ 0 ]) print \"Testing set has {} samples.\" . format ( X_test . shape [ 0 ]) Training set has 36177 samples. Testing set has 9045 samples. Establishing a Baseline Model If we create a mode always predicted an individual made more than \\$50,000, what would that model's accuracy and F-score be on this dataset? Let's test this out to serve as a baseline to compare our other learning algorithms's performance. from sklearn.metrics import confusion_matrix from sklearn.metrics import accuracy_score #naive classifier y_pred = income . replace ( 0 , 1 ) #confusion matrix cm = confusion_matrix ( income , y_pred ) #metrics tn = cm [ 0 , 0 ] fp = cm [ 0 , 1 ] fn = cm [ 1 , 0 ] tp = cm [ 1 , 1 ] recall = tp / ( tp + fn ) precision = tp / ( tp + fp ) beta = . 5 # Calculate accuracy accuracy = ( tp + tn ) / ( tp + tn + fp + fn ) # Calculate F-score using the formula above for beta = 0.5 fscore = ( 1 + beta ** 2 ) * (( precision * recall ) / (( beta ** 2 * precision ) + recall )) # Print the results print \"Naive Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]\" . format ( accuracy , fscore ) Naive Predictor : [ Accuracy score : 0.2478 , F - score : 0.2917 ] As expected, the Naive Predictor's accuracy is equal to the number of individuals making over \\$50K. Let's see if we can create a model which does a better job at predicting! Classification & Training — Creating a Modeling Pipeline from sklearn.metrics import accuracy_score , fbeta_score #metrics / scoring from sklearn.naive_bayes import GaussianNB from time import time def train_predict ( learner , sample_size , X_train , y_train , X_test , y_test ): ''' inputs: - learner: the learning algorithm to be trained and predicted on - sample_size: the size of samples (number) to be drawn from training set - X_train: features training set - y_train: income training set - X_test: features testing set - y_test: income testing set ''' results = {} # Fit the learner to the training data using slicing with 'sample_size' start = time () # Get start time learner = learner . fit ( X_train [: sample_size ], y_train [: sample_size ]) #sample_weight=sample_size end = time () # Get end time # Calculate the training time results [ 'train_time' ] = ( end - start ) # Get the predictions on the test set, # then get predictions on the first 300 training samples start = time () # Get start time predictions_test = learner . predict ( X_test ) #pred = clf.predict(features_test) predictions_train = learner . predict ( X_train [: 300 ]) end = time () # Get end time # Total prediction time results [ 'pred_time' ] = ( end - start ) # Compute accuracy on 300 training samples results [ 'acc_train' ] = accuracy_score ( y_train [: 300 ], predictions_train ) # Compute accuracy on test set results [ 'acc_test' ] = accuracy_score ( y_test , predictions_test ) # Compute F-score on 300 training samples results [ 'f_train' ] = fbeta_score ( y_train [: 300 ], predictions_train , beta =. 5 ) # Compute F-score on the test set results [ 'f_test' ] = fbeta_score ( y_test , predictions_test , beta =. 5 ) # Success print \"{} trained on {} samples.\" . format ( learner . __class__ . __name__ , sample_size ) # Return the results return results train_predict ( GaussianNB (), 36177 , X_train , y_train , X_test , y_test ) GaussianNB trained on 36177 samples. {'acc_test': 0.60829187396351581, 'acc_train': 0.59333333333333338, 'f_test': 0.42811288507232487, 'f_train': 0.41249999999999998, 'pred_time': 0.028007984161376953, 'train_time': 0.12101292610168457} Selecting Algorithm Candidates Application Example Strengths Weaknesses Why this is a good model for this specific problem Gaussian Naive Bayes (GaussianNB) - \"Naive Bayes spam filtering is a baseline technique for dealing with spam that can tailor itself to the email needs of individual users and give low false positive spam detection rates that are generally acceptable to users. It is one of the oldest ways of doing spam filtering, with roots in the 1990s.\" Source Requires a small amount of training data to train an algorithm Extremely quick training time Decent at classifying but bad at predicting Since we're trying to bucket individuals into income brackets NB's algorithm is a good candidate. If we were trying to predict an individual's exact income I would be less inclined to use this algorithm. Logistic Regression - \"Logistic regression is used in various fields, including machine learning, most medical fields, and social sciences.\" It lends itself particularly well to binary classification tasks common in medical research, such as classifying whether someone has a certain illness or not.\"For example, the Trauma and Injury Severity Score (TRISS), which is widely used to predict mortality in injured patients, was originally developed by Boyd et al. using logistic regression.\" Source High interpretability No parameter tuning necessary Requires a small amount of training data to train an algorithm Because Logistic Regression is such an interpretable model, it is often subject to high bias. Logistic Regression is one of the most basic flavors of classification algorithms and since that's exactly the task at hand it appears to be a good candidate to test. Random Forest - \"Or random decision forests[1][2] are an ensemble learning method for classification, that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.\" Random Forests are one of the most flexible and elegant ML algorithms and they have been applied to a wide range of fields including predicting the quality of surveys using NLP Source Relatively quick to train Excellent at predicting/classifying on all types of data without overfitting Slowest to run compared to other two methods Black Box model meaning we can't really see what's happening under the hood Random Forests algorithm is the bread and butter for many machine learning tasks due to its flexibility and ability to correct individual decision trees proneness to overfitting by sampling many of them. I hypothesize this algorithm will out perform the other two. Model Comparison / Evaluation Great now that we've estabilished a modeling pipeline, established baseline performance and selecte two other algorithms we can now, finally, get to modeling. # Import the three supervised learning models from sklearn from sklearn.naive_bayes import GaussianNB from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier , ExtraTreesClassifier #2 creating classifier # Initialize the three models clf_A = GaussianNB () clf_B = LogisticRegression ( random_state = 0 ) clf_C = RandomForestClassifier ( random_state = 0 ) # Calculate the number of samples for 1%, 10%, and 100% of the training data samples_1 = X_train . shape [ 0 ] * 0.01 ; samples_1 = int ( samples_1 ) samples_10 = X_train . shape [ 0 ] * 0.1 ; samples_10 = int ( samples_10 ) samples_100 = X_train . shape [ 0 ]; samples_100 = int ( samples_100 ) # Collect results on the learners results = {} for clf in [ clf_A , clf_B , clf_C ]: clf_name = clf . __class__ . __name__ results [ clf_name ] = {} for i , samples in enumerate ([ samples_1 , samples_10 , samples_100 ]): results [ clf_name ][ i ] = \\ train_predict ( clf , samples , X_train , y_train , X_test , y_test ) # Run metrics visualization for the three supervised learning models chosen vs . evaluate ( results , accuracy , fscore ) GaussianNB trained on 361 samples. GaussianNB trained on 3617 samples. GaussianNB trained on 36177 samples. LogisticRegression trained on 361 samples. LogisticRegression trained on 3617 samples. LogisticRegression trained on 36177 samples. RandomForestClassifier trained on 361 samples. RandomForestClassifier trained on 3617 samples. RandomForestClassifier trained on 36177 samples. Model Selection If we care most about the accuracy and speed of our classifier then based on our analysis of the three learning algorithms tested, I recommend we move forward with Logistic Regression because it outperforms both Naive Bayes and Random Forest in terms of prediction accuracy on our testing set and is much faster to train than random forest. Furthermore, it's F-score falls between the other two algorithms meaning we'd expect it to fall in the middle of our other two candidates on precision and recall. LOGISTIC REGRESSION What's happening under-the-hood of logit regression ? We can think of a logit regression as being similar to a linear regression but with a discrete as opposed to continuous output. The classification algorithm draws an S-shaped decision boundary which, in our case, divides the two classes. For each sample within our dataset, the algorithm assigns a probability to determine which class the point would fall into. For example if we have two classes (0 = less than $50K & 1 = more than $50K) , then (without any tuning of the decision boundary) points with a probability of less than .5 would be classified as 0 and points with greater than .5 probability would be classified as 1. TL;DR: In this scenario Logistic Regression provides a model that is more accurate, easy to interpret and quicker to predict on our testing data. Summary So Far We rigorously tested three machine learning algorithms to determine which would predict a person's level of income and after our analysis we feel confident in moving forward with logistic regression because it provides high accuracy, ease of interpretability (we get coefficients for each predictor) and is able to quickly provide predictions. In order to get this far we split our data into training and testing sets. But prior to this we normalized our targets by performing a log transformation on numeric target data, min-max scaling on all numeric predictor variables and one-hot-encoding on all categorical predictor features. We then trained our models using different cuts of training data (1%, 10% and 100% of the sets) and at each cut logistic regression provided the most accurate results. After the model was trained we used our testing test to determine accuracy and F-scores of each model on data it had not been previously trained on. At this step Logistic regression fell in the middle of the other two algorithms in terms of precision and recall, meaning it had the most neutral bias-variance trade-off. Model Tuning Not that we've selected Logistic Regression as our model of choice, we can use sklearn's GridSearchCV function to tune our model using different permutations of parameters to our model # Import 'GridSearchCV', 'make_scorer', and any other necessary libraries from sklearn.metrics import accuracy_score , fbeta_score , make_scorer from sklearn.linear_model import LogisticRegression , LogisticRegressionCV from sklearn.grid_search import GridSearchCV # Initialize the classifier clf = LogisticRegression () # Create the parameters list you wish to tune parameters = [{ 'C' : [ 0.01 , 0.1 , 1 , 10 ], \"solver\" : [ 'newton-cg' , 'liblinear' ]}] # Make an fbeta_score scoring object scorer = make_scorer ( fbeta_score , beta =. 5 ) # Perform grid search on the classifier using 'scorer' as the scoring method grid_obj = GridSearchCV ( LogisticRegression ( penalty = 'l2' , random_state = 0 ), parameters , scoring = scorer ) # Fit the grid search object to the training data and find the optimal parameters grid_fit = grid_obj . fit ( X_train , y_train ) # Get the estimator best_clf = grid_fit . best_estimator_ # Make predictions using the unoptimized and model predictions = ( clf . fit ( X_train , y_train )) . predict ( X_test ) best_predictions = best_clf . predict ( X_test ) # Report the before-and-afterscores print \"Unoptimized model \\n ------\" print \"Accuracy score on testing data: {:.4f}\" . format ( accuracy_score ( y_test , predictions )) print \"F-score on testing data: {:.4f}\" . format ( fbeta_score ( y_test , predictions , beta = 0.5 )) print \" \\n Optimized Model \\n ------\" print \"Final accuracy score on the testing data: {:.4f}\" . format ( accuracy_score ( y_test , best_predictions )) print \"Final F-score on the testing data: {:.4f}\" . format ( fbeta_score ( y_test , best_predictions , beta = 0.5 )) Unoptimized model ------ Accuracy score on testing data: 0.8483 F-score on testing data: 0.6993 Optimized Model ------ Final accuracy score on the testing data: 0.8498 Final F-score on the testing data: 0.7018 Final Model Evaluation Logistic Regression Optimized Results vs Naive Predictor's Baseline Results: Metric Benchmark Predictor Unoptimized Model Optimized Model Accuracy Score 24.38% 84.83% 84.94% F-score .2442 .6993 .7008 As we can see from the comparison table above, our logistic model does a much better job at accurately predicting an individual's income and has a higher F-Score which means it does a better job of balancing precision and recall. Extracting Feature Importance Here we'll choose a different scikit-learn supervised learning algorithm that has a feature_importance_ attribute. This attribute is a function that ranks the importance of each feature when making predictions based on the chosen algorithm so we can gain an understanding of the underlying importance for each feature within our model. # Import a supervised learning model that has 'feature_importances_' from sklearn.ensemble import AdaBoostClassifier # Train the supervised model on the training set model = AdaBoostClassifier ( n_estimators = 100 ) . fit ( X_train , y_train ) # Extract the feature importances importances = model . feature_importances_ # Plot vs . feature_plot ( importances , X_train , y_train ) Interestingly capital-gain/loss, age, hours-per-week and education explain the most variane within the dataset. Also interesting to note that the fives features here together account for ~60% of the total weight given to all predictors. Feature Selection How does the model perform if we only use a subset of all the available features in the data? With less features required to train, the time requird for training and prediction time is much lower — at the cost of performance metrics. From the viz above, we see that the top five most important features explain about ~60% of the variance within the dataset. This means that we can potentially compress our feature space and simplify the information required for the model to learn. This code will compare our optimized model with a full feature set to a new optimized model using only the top 5 features. # Import functionality for cloning a model from sklearn.base import clone # Reduce the feature space X_train_reduced = X_train [ X_train . columns . values [( np . argsort ( importances )[:: - 1 ])[: 5 ]]] X_test_reduced = X_test [ X_test . columns . values [( np . argsort ( importances )[:: - 1 ])[: 5 ]]] # Train on the \"best\" model found from grid search earlier clf = ( clone ( best_clf )) . fit ( X_train_reduced , y_train ) # Make new predictions reduced_predictions = clf . predict ( X_test_reduced ) # Report scores from the final model using both versions of data print \"Final Model trained on full data \\n ------\" print \"Accuracy on testing data: {:.4f}\" . format ( accuracy_score ( y_test , best_predictions )) print \"F-score on testing data: {:.4f}\" . format ( fbeta_score ( y_test , best_predictions , beta = 0.5 )) print \" \\n Final Model trained on reduced data \\n ------\" print \"Accuracy on testing data: {:.4f}\" . format ( accuracy_score ( y_test , reduced_predictions )) print \"F-score on testing data: {:.4f}\" . format ( fbeta_score ( y_test , reduced_predictions , beta =. 5 )) Final Model trained on full data ------ Accuracy on testing data: 0.8498 F-score on testing data: 0.7018 Final Model trained on reduced data ------ Accuracy on testing data: 0.8092 F-score on testing data: 0.5998 Effects of Feature Compression By reducing the number of predictor features we lose some accuracy (we go from 84.92% to 80.98%) and the F-score goes from .7 to .6. These differences are subtle so we'd need to understand the final use-case for the model to determine which one will better suite our needs. By performing the reduction we'd expect our model training and prediction times to decrease. If the goal of our model is to produce the most accurate results we'd likely want to move forward with the full model, in contrast if the goal is to create an more parsimonious model we'd favor the smaller, more-lightweight model. Thanks for reading, please reach out with comments on twitter !","tags":"Projects","loc":"https://etav.github.io/projects/classification_pipeline.html","title":"Classifying Charity Donors"},{"url":"https://etav.github.io/algorithms/linear_regression.html","text":"Introduction to Linear Regression Linear Regression or Ordinary Least Squares Regression (OLS) is one of the simplest machine learning algorithms and produces both accurate and interpretable results on most types of continuous data. While more sophisticated algorithms like random forest will produce more accurate results, they are know as \"black box\" models because it's tough for analysts to interpret the model. In contrast, OLS regression results are clearly interpretable because each predictor value (beta) is assigned a numeric value (coefficient) and a measure of significance for that variable (p-value). This allows the analyst to interpret the effect of difference predictors on the model and tune it easily. Here we'll use college admissions data and the statsmodels package to perform a simple linear regression looking at the relationship between average SAT score, out-of-state tuition and the selectivity for a range of US higher education institutions. We'll read the data using pandas and represent it visually using matplotlib . import pandas as pd import statsmodels.api as sm import matplotlib.pyplot as plt % matplotlib inline cols = [ 'ADM_RATE' , 'SAT_AVG' , 'TUITIONFEE_OUT' ] #cols to read, admit rate, avg sat score & out-of-state tuition df = pd . read_csv ( 'college_stats.csv' , usecols = cols ) df . dropna ( how = 'any' , inplace = True ) len ( df ) #1303 schools 1303 Represent the OLS Results Numerically #fit X & y y , X = ( df [ 'TUITIONFEE_OUT' ], df [[ 'SAT_AVG' , 'ADM_RATE' ]]) #call the model model = sm . OLS ( y , X ) #fit the model results = model . fit () #view results results . summary () OLS Regression Results Dep. Variable: TUITIONFEE_OUT R-squared: 0.919 Model: OLS Adj. R-squared: 0.919 Method: Least Squares F-statistic: 7355. Date: Sat, 24 Jun 2017 Prob (F-statistic): 0.00 Time: 12:11:48 Log-Likelihood: -13506. No. Observations: 1303 AIC: 2.702e+04 Df Residuals: 1301 BIC: 2.703e+04 Df Model: 2 Covariance Type: nonrobust coef std err t P>|t| [95.0% Conf. Int.] SAT_AVG 29.8260 0.577 51.699 0.000 28.694 30.958 ADM_RATE -9600.6540 907.039 -10.585 0.000 -1.14e+04 -7821.235 Omnibus: 9.845 Durbin-Watson: 1.313 Prob(Omnibus): 0.007 Jarque-Bera (JB): 7.664 Skew: -0.090 Prob(JB): 0.0217 Kurtosis: 2.670 Cond. No. 4.55e+03 Note that although we only used two variables we get a strong R-squared. This means that much of the variability within the out-of-state tuition can be explained or captured by SAT scores and selectivity or admittance rate. Represent the OLS Results Visually Plot of Out of State Tuition and Average SAT Score fig , ax = plt . subplots () fig = sm . graphics . plot_fit ( results , 0 , ax = ax ) ax . set_ylabel ( \"Out of State Tuition\" ) ax . set_xlabel ( \"Avg SAT Score\" ) ax . set_title ( \"OLS Regression\" ) <matplotlib.text.Text at 0x10b6cd790> Plot of Out of State Tuition and Admittance Rate fig , ax = plt . subplots () fig = sm . graphics . plot_fit ( results , 1 , ax = ax ) ax . set_ylabel ( \"Out of State Tuition\" ) ax . set_xlabel ( \"Admittance Rate\" ) ax . set_title ( \"OLS Regression\" ) <matplotlib.text.Text at 0x10cfd4390>","tags":"Algorithms","loc":"https://etav.github.io/algorithms/linear_regression.html","title":"Implementing Linear Regression using Stats Models"},{"url":"https://etav.github.io/projects/spam_message_classifier_naive_bayes.html","text":"1 Import Dataset The SMS spam dataset can be downloaded here . import pandas as pd # Dataset from - https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection df = pd . read_table ( 'smsspamcollection/SMSSpamCollection' , sep = ' \\t ' , header = None , names = [ 'label' , 'sms_message' ]) df . head () label sms_message 0 ham Go until jurong point, crazy.. Available only ... 1 ham Ok lar... Joking wif u oni... 2 spam Free entry in 2 a wkly comp to win FA Cup fina... 3 ham U dun say so early hor... U c already then say... 4 ham Nah I don't think he goes to usf, he lives aro... 1.1 Process the data set We need to transform the labels to binary values so we can run the regression. Here 1 = \"spam\" and 0 = \"ham\" #Map applies a function to all the items in an input list or df column. df [ 'label' ] = df . label . map ({ 'ham' : 0 , 'spam' : 1 }) df . head () label sms_message 0 0 Go until jurong point, crazy.. Available only ... 1 0 Ok lar... Joking wif u oni... 2 1 Free entry in 2 a wkly comp to win FA Cup fina... 3 0 U dun say so early hor... U c already then say... 4 0 Nah I don't think he goes to usf, he lives aro... 2.1 Enter Bag of Words Since we're dealing with text data and the naive bayes classifier is better suited to having numerical data as inputs we will need to perform transformations. To accomplish this we'll use the (\"bag of words\")[https://en.wikipedia.org/wiki/Bag-of-words_model] method to count the frequency of occurance for each word. Note: the bag of word methods assumes equal weight for all words in our \"bag\" and does not consider the order of occurance for words. There are modules that will do this for us but we will implement bag of words from scratch to understand what's happening under the hood. The steps are as follow: 1. Convert bag of words to lowercase. 2. Remove punctuation from sentences. 3. Break on each word. 4. Count the frequency of each word. import string #punctuation import pprint from collections import Counter #frequencies #Bag of Words from scratch documents = [ 'Hello, how are you!' , 'Win money, win from home.' , 'Call me now.' , 'Hello, Call hello you tomorrow?' ] lower_case_documents = [] for i in documents : lower_case_documents . append ( i . lower ()) print \"lower case:\" , lower_case_documents # Remove punctuation. sans_punctuation_documents = [] for i in lower_case_documents : sans_punctuation_documents = [ \"\" . join ( j for j in i if j not in string . punctuation ) for i in lower_case_documents ] print \"no punctuation:\" , ( sans_punctuation_documents ) #Break each word preprocessed_documents = [] for i in sans_punctuation_documents : preprocessed_documents . append ( i . split ( ' ' )) #split on space print \"break words:\" , ( preprocessed_documents ) #Count frequency of words using counter frequency_list = [] for i in preprocessed_documents : frequency_counts = Counter ( i ) frequency_list . append ( frequency_counts ) print \"tokenized counts:\" , pprint . pprint ( frequency_list ) lower case: ['hello, how are you!', 'win money, win from home.', 'call me now.', 'hello, call hello you tomorrow?'] no punctuation: ['hello how are you', 'win money win from home', 'call me now', 'hello call hello you tomorrow'] break words: [['hello', 'how', 'are', 'you'], ['win', 'money', 'win', 'from', 'home'], ['call', 'me', 'now'], ['hello', 'call', 'hello', 'you', 'tomorrow']] tokenized counts:[Counter({'how': 1, 'you': 1, 'hello': 1, 'are': 1}), Counter({'win': 2, 'home': 1, 'from': 1, 'money': 1}), Counter({'me': 1, 'now': 1, 'call': 1}), Counter({'hello': 2, 'you': 1, 'call': 1, 'tomorrow': 1})] None 2.2 SciKit-Learn Feature Extraction That was pretty simple but scikit-learn makes the process even easier. Let's try it using the sklearn.feature_extraction.text.CountVectorizer method from the module. from sklearn.feature_extraction.text import CountVectorizer count_vector = CountVectorizer () #set the variable count_vector . fit ( documents ) #fit the function count_vector . get_feature_names () #get the outputs [u'are', u'call', u'from', u'hello', u'home', u'how', u'me', u'money', u'now', u'tomorrow', u'win', u'you'] Create an array where each row represents one of the 4 columns and each column represents the counts for each word within the document. doc_array = count_vector . transform ( documents ) . toarray () doc_array array([[1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0], [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0], [0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1]]) Convert the array to a data frame and apply get_feature_names as the column names. frequency_matrix = pd . DataFrame ( doc_array , columns = count_vector . get_feature_names () ) frequency_matrix are call from hello home how me money now tomorrow win you 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 2 0 2 0 1 0 0 0 0 1 0 1 0 0 0 3 0 1 0 2 0 0 0 0 0 1 0 1 3.1 Training & Testing Sets We'll split our dataset using scikit's train_test_split method into training and testing sets so we can make inferences about the model's accuracy on data it hasn't been trained on. from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( df [ 'sms_message' ], df [ 'label' ], random_state = 1 ) print \"Our original set contains\" , df . shape [ 0 ], \"observations\" print \"Our training set contains\" , X_train . shape [ 0 ], \"observations\" print \"Our testing set contains\" , X_test . shape [ 0 ], \"observations\" Our original set contains 5572 observations Our training set contains 4179 observations Our testing set contains 1393 observations Fit the training & testing data to the CountVectorizer() method and return a matrix train = count_vector . fit_transform ( X_train ) test = count_vector . transform ( X_test ) 4.1 Implementing Baye's Theorem from Scratch Bayes' theorem calculates the probability of a given class or state given the joint-probability distribution of the input variables (betas). There are numerous libraries which take care of this for us native to python and R but in order to understand what's happening behind the scenes let's calculate bayes theorem from scratch. Here we'll create a fictitious world in which we're testing patients for HIV. P(HIV) = The odds of a person having HIV is .015 or 1.5% P(Positive) = The probability the test results are positive P(Negative) = The probability the test results are negative. P(Positive | HIV) = The probability the test results are positive given someone has HIV. This is also called Sensitivity or True Positive Rate. We'll assume the test is correct .95 or 95% of the time. P(Positive | ~HIV) = The probability the test results are positive given someone does not have HIV. This is also called Specificity or True Negative Rate. We'll assume this is also correct .95 or 95% of the time. Baye's Formula: Where: - P(A) is the probability of A occurring independently, for us this is P(HIV) . - P(B) is the probability of B occurring independently, for us this is P(Positive) . - P(A|B) is the posterior probability of A occurring given B occurs, for us this is P(HIV | Positive) . This is the probability that an individual has HIV given their test results are positive and what we're trying to calculate. - P(B|A) is the likelihood probability of B occurring, given A occurs. In our example this is P(Positive | HIV) . This value is given to us. Stringing these together we get: P(HIV | Positive) = ((P(HIV) * P(Positive | HIV)) / P(Positive) Thus the probability of getting a positive HIV test result P(HIV) becomes: P(Positive) = [P(HIV) * Sensitivity] + [P(~HIV) * (1-Specificity)] #performing calculations: p_hiv = . 015 #P(HIV) assuming 1.5% of the population has HIV p_no_hiv = . 98 # P(~HIV) p_positive_hiv = . 95 #sensitivity p_negative_hiv = . 95 #specificity #P(Positive) p_positive = ( p_hiv * p_positive_hiv ) + ( p_no_hiv * ( 1 - p_negative_hiv )) print \"The probability of getting a positive test result is:\" , p_positive , \"this is our prior\" The probability of getting a positive test result is: 0.06325 this is our prior Using this prior we can calculate our posterior probabilities as follows: The probability of an individual having HIV given their test result is positive. P(D|Positive) = (P(HIV) * Sensitivity)) / P(Positive) The probability of an individual not having HIV given their test result is positive. P(~D|Positive) = (P(~HIV) * (1-Sensitivity))) / P(Positive) Note: the sum of posteriors must equal one because combined they capture all possible states within our set of probabilities. #P(HIV | Positive) p_hiv_positive = ( p_hiv * p_positive_hiv ) / p_positive print \"The probability of a person having HIV, given a positive test result is:\" , p_hiv_positive The probability of a person having HIV, given a positive test result is: 0.225296442688 #P(~HIV | Positive) p_positive_no_hiv = 1 - p_positive_hiv p_no_hiv_positive = ( p_no_hiv * p_positive_no_hiv ) / p_positive print \"The probability of an individual not having HIV given getting a positive test result is:\" , p_no_hiv_positive The probability of an individual not having HIV given getting a positive test result is: 0.774703557312 That's it! We've just demonstrated how to calculate Bayes theorem from scratch. In our toy example we showed that if an individual gets a positive test result the probability this individual has HIV is 22.5% and 77.5% that they do not have HIV. We can check the validity of our results by summing the probability of both cases: posterior_sum = p_no_hiv_positive + p_hiv_positive posterior_sum #sum to 1, looks good! 1.0 5.1 Naive Bayes Classifier using Scikit-Learn In the above example we only the probability given two inputs (the test result and the status of the disease in the patient). This calculation would grow exponentially more complex given numerous inputs and would be painstaking to calculate by hand. Don't worry, SciKit-Learn is here to save the day (and a ton of time)! Our spam classifier will use multinomial naive Bayes method from sklearn.nive_bayes . This method is well-suited for for discrete inputs (like word counts) whereas the Gaussian Naive Bayes classifier performs better on continuous inputs. from sklearn.naive_bayes import MultinomialNB naive_bayes = MultinomialNB () #call the method naive_bayes . fit ( train , y_train ) #train the classifier on the training set MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) predictions = naive_bayes . predict ( test ) #predic using the model on the testing set 6.1 Evaluating the Model After training our model we're now ready to evaluate its accuracy and precision. Accuracy: A ratio of correct predictions to the total number of predictions. Precision: The proportion of messages which were correctly classified as spam. This is a ratio of true positives (messages classified as SPAM which actually are SPAM) to all positives (all messages classified as SPAM). from sklearn.metrics import accuracy_score , precision_score , f1_score print ( 'accuracy score: ' ), format ( accuracy_score ( y_test , predictions )) print ( 'precision score: ' ), format ( precision_score ( y_test , predictions )) accuracy score: 0.988513998564 precision score: 0.972067039106 Conclusion Through this excercise we learned how to implement bag of words and the naive bayes method first from scratch to gain insight into the technicalities of the methods and then again using scikit-learn to provide scalable results. We've learned that the naive bayes classifier can produce robust results without significant tuning to the model. Our final model classifies text messages as spam with 98.8% accuracy and 98.8% precision.","tags":"Projects","loc":"https://etav.github.io/projects/spam_message_classifier_naive_bayes.html","title":"Spam Classifier using Naive Bayes"},{"url":"https://etav.github.io/algorithms/bayes_theorem_scratch.html","text":"Implementing Bayes' Theorem from Scratch Bayes' theorem calculates the probability of a given class or state given the joint-probability distribution of the input variables (betas). There are numerous libraries which take care of this for us which are native to python and R but in order to understand what's happening \"behind the scenes\" we'll use Bayes' Theorem to calculate join probability distributions from scratch. Here we'll create a fictitious world in which we're a doctor testing patients for HIV, subject to the following assumptions: P(HIV) = The odds of a person having HIV is .015 or 1.5% P(Positive) = The probability the test results are positive P(Negative) = The probability the test results are negative. P(Positive | HIV) = The probability the test results are positive given someone has HIV. This is also called Sensitivity or True Positive Rate. We'll assume the test is correct .95 or 95% of the time. P(Negative | ~HIV) = The probability the test results are negative given someone does not have HIV. This is also called Specificity or True Negative Rate. We'll assume this is also correct .95 or 95% of the time. Bayes' Formula: Where: P(A) is the prior probability of A occurring independently, for us this is P(HIV) . P(B) is the prior probability of B occurring independently, for us this is P(Positive) . P(A|B) is the posterior probability of A occurring given B occurs, for us this is P(HIV | Positive) . This is the probability that an individual has HIV given their test results are positive and what we're trying to calculate. P(B|A) is the likelihood probability of B occurring, given A occurs. In our example this is P(Positive | HIV) . This value is given to us. Stringing these together we get: P(HIV | Positive) = ((P(HIV) * P(Positive | HIV)) / P(Positive) Thus the probability of getting a positive HIV test result P(HIV) becomes: P(Positive) = [P(HIV) * Sensitivity] + [P(~HIV) * (1-Specificity)] Calculations - Priors #performing calculations: p_hiv = . 015 #P(HIV) assuming 1.5% of the population has HIV p_no_hiv = . 98 # P(~HIV) p_positive_hiv = . 95 #sensitivity p_negative_hiv = . 95 #specificity #P(Positive) p_positive = ( p_hiv * p_positive_hiv ) + ( p_no_hiv * ( 1 - p_negative_hiv )) print \"The probability of getting a positive test result is:\" , p_positive , \"this is our prior\" The probability of getting a positive test result is: 0.06325 this is our prior Using this prior we can calculate our posterior probabilities as follows: The probability of an individual having HIV given their test result is positive. P(HIV|Positive) = (P(HIV) * Sensitivity)) / P(Positive) The probability of an individual not having HIV given their test result is positive. P(~HIV|Positive) = (P(~HIV) * (1-Sensitivity))) / P(Positive) Note: the sum of posteriors must equal one because combined they capture all possible states within our set of probabilities. Calculations - Posteriors #P(HIV | Positive) p_hiv_positive = ( p_hiv * p_positive_hiv ) / p_positive print \"The probability of a person having HIV, given a positive test result is:\" , p_hiv_positive The probability of a person having HIV, given a positive test result is: 0.225296442688 #P(~HIV | Positive) p_positive_no_hiv = 1 - p_positive_hiv p_no_hiv_positive = ( p_no_hiv * p_positive_no_hiv ) / p_positive print \"The probability of an individual not having HIV a positive test result is:\" , p_no_hiv_positive The probability of an individual not having HIV a positive test result is: 0.774703557312 Conclusion That's it! We've just demonstrated how to calculate Bayes' theorem from scratch. In our illustrative example we showed that if an individual gets a positive test result the probability this individual has HIV is 22.5% and 77.5% that they do not have HIV. We can check the validity of our results by summing the probability of both cases: posterior_sum = p_no_hiv_positive + p_hiv_positive posterior_sum #sum to 1, looks good! 1.0","tags":"Algorithms","loc":"https://etav.github.io/algorithms/bayes_theorem_scratch.html","title":"Implementing Bayes' Theorem from Scratch"},{"url":"https://etav.github.io/projects/random_forest_college_admissions.html","text":"College Admissions Exploratory Project in R 1. Introduction Matching high school students to colleges which will fit them well is a primary duties of high school guidance counselors. As the competition for jobs increases, attaining a bachelor's degree is a good way to differentiate and refine skills while providing time for students to mature into productive young adults. The main barrier to entry for many students is the hefty price tag of a college degree which has increased by 1,128% since 1978 (Bloomberg) . Currently the College Board reports the average private non-profit four-year tuition is $32,405 however, this price does factor in the cost of living for students which can greatly inflate this figure. The Wall Street Journal estimates that the average college graduate in 2015 will have $35,000 in student debt which is the highest level on record while the total dollar value attributable to student-loans in the US is rapidly approaching $70 billion . After considering both the potential future pay-off and the high-cost of pursuing a college education, the seriousness of this decision becomes obvious. Like many modern decision systems, the college application process relies heavily on data to make informed admissions decisions. As such, within the scope of this essay we will explore a data set from a specific college's admission office and use Random Forests from the CRAN() library to create a model which given inputs (ie: demographic information and scholastic metrics like GPA) will create a model to predict the likelihood of being admitted into a specific secondary institution. 2. The Data Our data set contains 8,700 observations and 9 variables. The data comes from a specific university's application office and each row contains variables related to the admission decision, the student's scholastic performance and other demographic information. In this section we will provide an overview of each variable before moving onto univariate analysis. Variable Name Description admit A binary factor where \"1\" indicates the student was admitted and \"0\" indicates a rejection. This is our response variable. anglo A binary factor where \"1\" indicates the student is Caucasian. asian A binary factor where \"1\" indicates the student is Asian. black A binary factor where \"1\" indicates the student is African American. gpa.weighted A numerical variable which provides a weighted GPA of the student (may be greater than 4.0 if the student was enrolled in AP courses). sati-verb A numerical variable capturing the student's verbal SAT score. sati-math A numerical variable capturing the student's math SAT score. income A numerical variable capturing the student's house-hold income, it has been capped at $100,000. sex A binary factor where \"1\" indicates the student is male. 2.1 Partitioning the Data: Training, Testing & Evaluation Sets For this implementation of the random forest algorithm we will not worry about creating training, testing and evaluation data sets because the randomForest function has a built-in OOB estimator which we can use to determine its performance and removing the necessity to set aside a training set. 2.2 Univariate Analysis In order to get a high-level understanding of our data we will examine the summary statistics conveniently provided by the R's summary() function before looking into any variable more specifically. Here's the output from the function: We will now comment on each variable independently. As we can tell from the output above, 2686 of the 8700 applicants were accepted which means admission rate for this university is: 2686/8700 = ~30.89% When comparing the admission rate for this university (31%) to that of the average 4-Year Private University (62.8%) we learn that it is twice as selective ( source ). Because all three of our our predictor variables related to a student's race capture the same type of information we will analyze them together. We have ethnic data for 6583 or approximately 76% of our test sample, we have 829 observations where there is missing ethnic data. Race Count Proportion (% of Total) Asian 3417 39% African American 356 4% Caucasian 2810 32% NA 856 10% Other 1261 15% Total 8700 100% Since the student's weighted GPA, SAT verbal and SAT Math scores are all numeric data-types, we will create histograms to analyze the distribution of each variable checking for normality and any potential outliers: Without performing sophisticated analysis we can tell these data look fairly normally distributed. However, we must note the large count of \"0s\" for SAT scores. Upon investigating it appears that we have missing data for 457 of our sample SAT scores. This should not cause too much concern because it is a relatively small proportion of our total sample. Furthermore, we should not remove the data because we can't tell if it is missing randomly or if there was systematic recording error present. We will keep these missing values in mind when we move on to model building. The final variable we will examine is house hold income. It is important to note that all house holds have been capped at an income of $100,000 , meaning that all income levels above this cut-off point will be labeled as \"$100,000\". This has a strong effect which becomes evident when we create a histogram of for this variable. ## 3. Multivariate Analysis We will limit the exploration of multivariate graphs and statistical relationships to numerical data, or specifically: SAT Math and SAT verbal scores, weighted GPA and household income . However, since Math SAT scores and Verbal SAT scores are strongly correlated (R-squared =.736), and thus explain similiar variance within the dataset we will only consider Math scores. Below are summary plots to visualize the relationships between our numerical variables. ## 4. Random Forests Background 4.1 Technicalities R's CART (Classification And Regression Trees) package which we will use to call the RandomForest function uses the following equation to define a splitting decision: ΔI(s,A) = I(A) - p(A_L)I(A_L) - p(A_R)I(A_R) Where I(A) is the value of \"impurity\" in a parent node, p(A_L) is the likelihood of a given case falling to the left of the parent node and I(A_L) is the impurity of the daughter node resulting from the split, while P(A_R) and I(A_R) capture similar information but for the right daughter node. Given the above formula the CART algorithm selects a predictor which maximizes ΔI(s,A). By doing so the algorithm is creating terminal nodes which look as homogeneous as possible within the node but as heterogeneous as possible when compared to other terminal nodes . By now it should become clear that Random Forests are simply a large number of decision trees, each node of which is subject to the above optimization ( CART Docs ). In reality, the formula for creating a splitting decision is the same for CART as RandomForest , the key difference is that RandomForest creates numerous decision trees sacrificing interpretability for increased predictive power. 5. Model Building With Random Forests After performing our variable exploration, univariate and bivarate analysis I feel comfortable moving on to building a Random Forest classifier using our data set. Without any model tuning, the algorithm provides the following confusion matrix: State Predicted: Not Admitted Predicted: Admitted Model Error Not Admitted (\"0\") 4240 308 6.7% Admitted (\"1\") 716 1240 36.6% Use Error 14.5% 19.9% Overall Error = 15.7% We have created a collection or \"forest\" featuring 500 decision trees. In this scenario we have not tuned our parameters and we will accept the default values for priors, this will serve as our baseline model which we can improve upon by tuning priors. The Out of Bag estimate for error rate provided is 15.7%. We also note that our model is under predicting the number of students which should be classified as being admitted (our model predicts 23.8% of students will be admitted but we know from our univariate analysis in section 2.2 that the admit rate should be closer to 31%). 5.1 Calling Random Forests #-----MODEL BUILDING: RANDOM FORESTS CALL----- library ( randomForest ) # random forests rf1 <- randomForest ( admit ~ . , data = data , importance = T , na.action = na.fail , ntree = 500 ) rf1 5.2 Model Tuning To correct for the algorithm under predicting the total number of students admitted we introduce priors and assign a cost ratio of 3 to 1 . What this means is that we are giving the algorithm \"slack\" while predicting whether a student is admitted and tightening our threshold for predicting whether someone is not admitted. After this modification we would expect a false positive rate 3 times greater than a false negative rate . The reason for selecting this ratio is because it seems more costly to fail to predict when someone is admitted compared to the scenario in which a student is classified as being admitted but was actually not.After running our entire data set through the randomForest() algorithm and setting our cost ration equal to 3 to 1, we arrive at the confusion matrix below: State Predicted: Not Admitted Predicted: Admitted Model Error Not Admitted (\"0\") 3536 1012 22.2% Admitted (\"1\") 294 1662 15% Use Error 7.7% 37.9% Overall Error = 20% Notice how the forecasted total number of students admitted went up, increasing the use error but decreasing the model error. 5.3 Model Evaluation As we see from the plot below the most important variables for predicting whether a student is admitted to this university are those related to academic achievement. For example, Weighted GPA, SAT Math and SAT Verbal Scores explain the most variance within the dataset for our model . While factors like income and race contributed to the accuracy, their effect was less pronounced in our model. We created partial dependency plots for each of our quantitative variables. The plots for weighted GPA begins to increase at 2.0 and peak around 3.5 meaning that values of weighted GPA between 3 and 3.5 tend to predict \"Admitted = True\" strongly. For both SAT verbal and SAT Math scores we see a similar trend. The odds of being admitted increase as levels of income reach above $90,000, see the partial dependency plots below for more details: 5.4 Conclusion Through this analysis we have gained a deeper understanding into the data behind college admissions. By analyzing the variables available to us we were able to determine that variables related to scholastic performance are much better for predicting the admittance rate of an individual student when compared with socioeconomic and racial factors . We tuned our model to a 3 to 1 cost ratio for false positives and false negatives, meaning we penalized the model by a factor of 3 to 1 when predicting whether a student was accepted. Initially the baseline model produced too many false negatives which was something I had not anticipated. For future analysis I would like to have a larger dataset which includes more than 1 university.","tags":"Projects","loc":"https://etav.github.io/projects/random_forest_college_admissions.html","title":"College Admissions Exploratory Analysis in R"},{"url":"https://etav.github.io/python/count_basic_freq_plot.html","text":"Counting is an essential task required for most analysis projects. The ability to take counts and visualize them graphically using frequency plots (histograms) enables the analyst to easily recognize patterns and relationships within the data. Good news is this can be accomplished using python with just 1 line of code! import pandas as pd % matplotlib inline df = pd . read_csv ( 'iris-data.csv' ) #toy dataset df . head () sepal_length_cm sepal_width_cm petal_length_cm petal_width_cm class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa df [ 'class' ][: 5 ] 0 Iris-setosa 1 Iris-setosa 2 Iris-setosa 3 Iris-setosa 4 Iris-setosa Name: class, dtype: object Frequency Plot for Categorical Data df [ 'class' ] . value_counts () #generate counts Iris-virginica 50 Iris-setosa 49 Iris-versicolor 45 versicolor 5 Iris-setossa 1 Name: class, dtype: int64 Notice that the value_counts() function automatically provides the classes in decending order. Let's bring it to life with a frequency plot. df [ 'class' ] . value_counts () . plot () I think a bar graph would be more useful, visually. df [ 'class' ] . value_counts () . plot ( 'bar' ) df [ 'class' ] . value_counts () . plot ( 'barh' ) #horizontal bar plot df [ 'class' ] . value_counts () . plot ( 'barh' ) . invert_yaxis () #horizontal bar plot There you have it, a ranked bar plot for categorical data in just 1 line of code using python! Histograms for Numberical Data You know how to graph categorical data, luckily graphing numerical data is even easier using the hist() function. df [ 'sepal_length_cm' ] . hist () #horizontal bar plot df [ 'sepal_length_cm' ] . hist ( bins = 30 ) #add granularity df [ 'sepal_length_cm' ] . hist ( bins = 30 , range = [ 4 , 8 ]) #add granularity & range df [ 'sepal_length_cm' ] . hist ( bins = 30 , range = [ 4 , 8 ], facecolor = 'gray' ) #add granularity & range & color There you have it, a stylized histogram for numerical data using python in 1 compact line of code.","tags":"Python","loc":"https://etav.github.io/python/count_basic_freq_plot.html","title":"Counting and Basic Frequency Plots"},{"url":"https://etav.github.io/python/python_pandas_rows_cols.html","text":"Slicing dataframes by rows and columns is a basic tool every analyst should have in their skill-set. We'll run through a quick tutorial covering the basics of selecting rows, columns and both rows and columns.This is an extremely lightweight introduction to rows, columns and pandas—perfect for beginners! Import Dataset import pandas as pd df = pd . read_csv ( 'iris-data.csv' ) df . head () sepal_length_cm sepal_width_cm petal_length_cm petal_width_cm class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa df . shape (150, 5) Selecting the first ten rows df [: 10 ] sepal_length_cm sepal_width_cm petal_length_cm petal_width_cm class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa 5 5.4 3.9 1.7 0.4 Iris-setosa 6 4.6 3.4 1.4 0.3 Iris-setosa 7 5.0 3.4 1.5 NaN Iris-setosa 8 4.4 2.9 1.4 NaN Iris-setosa 9 4.9 3.1 1.5 NaN Iris-setosa selecting the last five rows df [ - 5 :] sepal_length_cm sepal_width_cm petal_length_cm petal_width_cm class 145 6.7 3.0 5.2 2.3 Iris-virginica 146 6.3 2.5 5.0 2.3 Iris-virginica 147 6.5 3.0 5.2 2.0 Iris-virginica 148 6.2 3.4 5.4 2.3 Iris-virginica 149 5.9 3.0 5.1 1.8 Iris-virginica Selecting rows 15-20 df [ 15 : 20 ] sepal_length_cm sepal_width_cm petal_length_cm petal_width_cm class 15 5.7 4.4 1.5 0.4 Iris-setosa 16 5.4 3.9 1.3 0.4 Iris-setosa 17 5.1 3.5 1.4 0.3 Iris-setosa 18 5.7 3.8 1.7 0.3 Iris-setossa 19 5.1 3.8 1.5 0.3 Iris-setosa Selecting Columns The quickest way to do this using pandas is by providing the column name as the input: df [ 'class' ] 0 Iris-setosa 1 Iris-setosa 2 Iris-setosa 3 Iris-setosa 4 Iris-setosa 5 Iris-setosa 6 Iris-setosa 7 Iris-setosa 8 Iris-setosa 9 Iris-setosa 10 Iris-setosa 11 Iris-setosa 12 Iris-setosa 13 Iris-setosa 14 Iris-setosa 15 Iris-setosa 16 Iris-setosa 17 Iris-setosa 18 Iris-setossa 19 Iris-setosa 20 Iris-setosa 21 Iris-setosa 22 Iris-setosa 23 Iris-setosa 24 Iris-setosa 25 Iris-setosa 26 Iris-setosa 27 Iris-setosa 28 Iris-setosa 29 Iris-setosa ... 120 Iris-virginica 121 Iris-virginica 122 Iris-virginica 123 Iris-virginica 124 Iris-virginica 125 Iris-virginica 126 Iris-virginica 127 Iris-virginica 128 Iris-virginica 129 Iris-virginica 130 Iris-virginica 131 Iris-virginica 132 Iris-virginica 133 Iris-virginica 134 Iris-virginica 135 Iris-virginica 136 Iris-virginica 137 Iris-virginica 138 Iris-virginica 139 Iris-virginica 140 Iris-virginica 141 Iris-virginica 142 Iris-virginica 143 Iris-virginica 144 Iris-virginica 145 Iris-virginica 146 Iris-virginica 147 Iris-virginica 148 Iris-virginica 149 Iris-virginica Name: class, dtype: object df [[ 'class' , 'petal_width_cm' ]] #two columns class petal_width_cm 0 Iris-setosa 0.2 1 Iris-setosa 0.2 2 Iris-setosa 0.2 3 Iris-setosa 0.2 4 Iris-setosa 0.2 5 Iris-setosa 0.4 6 Iris-setosa 0.3 7 Iris-setosa NaN 8 Iris-setosa NaN 9 Iris-setosa NaN 10 Iris-setosa NaN 11 Iris-setosa NaN 12 Iris-setosa 0.1 13 Iris-setosa 0.1 14 Iris-setosa 0.2 15 Iris-setosa 0.4 16 Iris-setosa 0.4 17 Iris-setosa 0.3 18 Iris-setossa 0.3 19 Iris-setosa 0.3 20 Iris-setosa 0.2 21 Iris-setosa 0.4 22 Iris-setosa 0.2 23 Iris-setosa 0.5 24 Iris-setosa 0.2 25 Iris-setosa 0.2 26 Iris-setosa 0.4 27 Iris-setosa 0.2 28 Iris-setosa 0.2 29 Iris-setosa 0.2 ... ... ... 120 Iris-virginica 2.3 121 Iris-virginica 2.0 122 Iris-virginica 2.0 123 Iris-virginica 1.8 124 Iris-virginica 2.1 125 Iris-virginica 1.8 126 Iris-virginica 1.8 127 Iris-virginica 1.8 128 Iris-virginica 2.1 129 Iris-virginica 1.6 130 Iris-virginica 1.9 131 Iris-virginica 2.0 132 Iris-virginica 2.2 133 Iris-virginica 1.5 134 Iris-virginica 1.4 135 Iris-virginica 2.3 136 Iris-virginica 2.4 137 Iris-virginica 1.8 138 Iris-virginica 1.8 139 Iris-virginica 2.1 140 Iris-virginica 2.4 141 Iris-virginica 2.3 142 Iris-virginica 1.9 143 Iris-virginica 2.3 144 Iris-virginica 2.5 145 Iris-virginica 2.3 146 Iris-virginica 2.3 147 Iris-virginica 2.0 148 Iris-virginica 2.3 149 Iris-virginica 1.8 150 rows × 2 columns Selecting Rows & Columns df [ 'class' ][: 5 ] #just first 5 instances 0 Iris-setosa 1 Iris-setosa 2 Iris-setosa 3 Iris-setosa 4 Iris-setosa Name: class, dtype: object df [ df . columns [ 4 ]][ 5 : 10 ] #observations 5-10 using 'columns' 5 Iris-setosa 6 Iris-setosa 7 Iris-setosa 8 Iris-setosa 9 Iris-setosa Name: class, dtype: object df . ix [:, 4 ][ - 5 :] # last five observations of column using 'ix' 145 Iris-virginica 146 Iris-virginica 147 Iris-virginica 148 Iris-virginica 149 Iris-virginica Name: class, dtype: object","tags":"Python","loc":"https://etav.github.io/python/python_pandas_rows_cols.html","title":"Selecting Rows And Columns in Python Pandas"},{"url":"https://etav.github.io/python/vif_factor_python.html","text":"Colinearity is the state where two variables are highly correlated and contain similiar information about the variance within a given dataset. To detect colinearity among variables, simply create a correlation matrix and find variables with large absolute values. In R use the corr function and in python this can by accomplished by using numpy's corrcoef function. Multicolinearity on the other hand is more troublesome to detect because it emerges when three or more variables, which are highly correlated, are included within a model. To make matters worst multicolinearity can emerge even when isolated pairs of variables are not colinear. A common R function used for testing regression assumptions and specifically multicolinearity is \"VIF()\" and unlike many statistical concepts, its formula is straightforward: $$ V.I.F. = 1 / (1 - R&#94;2). $$ The Variance Inflation Factor (VIF) is a measure of colinearity among predictor variables within a multiple regression. It is calculated by taking the the ratio of the variance of all a given model's betas divide by the variane of a single beta if it were fit alone. Steps for Implementing VIF Run a multiple regression. Calculate the VIF factors. Inspect the factors for each predictor variable, if the VIF is between 5-10, multicolinearity is likely present and you should consider dropping the variable. #Imports import pandas as pd import numpy as np from patsy import dmatrices import statsmodels.api as sm from statsmodels.stats.outliers_influence import variance_inflation_factor df = pd . read_csv ( 'loan.csv' ) df . dropna () df = df . _get_numeric_data () #drop non-numeric cols df . head () id member_id loan_amnt funded_amnt funded_amnt_inv int_rate installment annual_inc dti delinq_2yrs ... total_bal_il il_util open_rv_12m open_rv_24m max_bal_bc all_util total_rev_hi_lim inq_fi total_cu_tl inq_last_12m 0 1077501 1296599 5000.0 5000.0 4975.0 10.65 162.87 24000.0 27.65 0.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 1077430 1314167 2500.0 2500.0 2500.0 15.27 59.83 30000.0 1.00 0.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2 1077175 1313524 2400.0 2400.0 2400.0 15.96 84.33 12252.0 8.72 0.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 1076863 1277178 10000.0 10000.0 10000.0 13.49 339.31 49200.0 20.00 0.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 1075358 1311748 3000.0 3000.0 3000.0 12.69 67.79 80000.0 17.94 0.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 5 rows × 51 columns df = df [[ 'annual_inc' , 'loan_amnt' , 'funded_amnt' , 'annual_inc' , 'dti' ]] . dropna () #subset the dataframe Step 1: Run a multiple regression %% capture #gather features features = \"+\" . join ( df . columns - [ \"annual_inc\" ]) # get y and X dataframes based on this regression: y , X = dmatrices ( 'annual_inc ~' + features , df , return_type = 'dataframe' ) Step 2: Calculate VIF Factors # For each X, calculate VIF and save in dataframe vif = pd . DataFrame () vif [ \"VIF Factor\" ] = [ variance_inflation_factor ( X . values , i ) for i in range ( X . shape [ 1 ])] vif [ \"features\" ] = X . columns Step 3: Inspect VIF Factors vif . round ( 1 ) VIF Factor features 0 5.1 Intercept 1 1.0 dti 2 678.4 funded_amnt 3 678.4 loan_amnt As expected, the total funded amount for the loan and the amount of the loan have a high variance inflation factor because they \"explain\" the same variance within this dataset. We would need to discard one of these variables before moving on to model building or risk building a model with high multicolinearity.","tags":"Python","loc":"https://etav.github.io/python/vif_factor_python.html","title":"Variance Inflation Factor (VIF) Explained"},{"url":"https://etav.github.io/python/scikit_pca.html","text":"Principal Component Analysis (PCA) in Python using Scikit-Learn Principal component analysis is a technique used to reduce the dimensionality of a data set. PCA is typically employed prior to implementing a machine learning algorithm because it minimizes the number of variables used to explain the maximum amount of variance for a given data set. PCA Introduction PCA uses \"orthogonal linear transformation\" to project the features of a data set onto a new coordinate system where the feature which explains the most variance is positioned at the first coordinate (thus becoming the first principal component). Source PCA allows us to quantify the trade-offs between the number of features we utilize and the total variance explained by the data. PCA allows us to determine which features capture similiar information and discard them to create a more parsimonious model. In order to perform PCA we need to do the following: PCA Steps Standardize the data. Use the standardized data to create a covariance matrix. Use the resulting matrix to calculate eigenvectors (principal components) and their corresponding eigenvalues. Sort the components in decending order by its eigenvalue. Choose n components which explain the most variance within the data (larger eigenvalue means the feature explains more variance). Create a new matrix using the n components. NOTE: PCA compresses the feature space so you will not be able to tell which variables explain the most variance because they have been transformed. If you'd like to preserve the original features to determine which ones explain the most variance for a given data set, see the SciKit Learn Feature Documentation . Resources 1. District data labs 2. chris 3. implementation 4. More #Imports import matplotlib.pyplot as plt import numpy as np import pandas as pd from sklearn import decomposition from sklearn.preprocessing import scale from sklearn.decomposition import PCA import seaborn as sb % matplotlib inline /Users/ernestt/venv/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment. warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.') sb . set ( font_scale = 1.2 , style = \"whitegrid\" ) #set styling preferences loan = pd . read_csv ( 'loan.csv' ) . sample ( frac = . 25 ) #read the dataset and sample 25% of it / Users / ernestt / venv / lib / python2 . 7 / site - packages / IPython / core / interactiveshell . py : 2717 : DtypeWarning : Columns ( 19 , 55 ) have mixed types . Specify dtype option on import or set low_memory = False . interactivity = interactivity , compiler = compiler , result = result ) For this example, we're going to use the Lending Club data set which can be found here . #Data Wrangling loan . replace ([ np . inf , - np . inf ], np . nan ) #convert infs to nans loan = loan . dropna ( axis = 1 , how = 'any' ) #remove nans loan = loan . _get_numeric_data () #keep only numeric features Step 1: Standardize the Dataset x = loan . values #convert the data into a numpy array x = scale ( x ); x array([[ 1.17990021, 1.17491004, -0.61220612, ..., -0.07607754, -0.38999916, 0. ], [ 1.57614469, 1.58965176, 0.14553604, ..., -0.07607754, -0.45317429, 0. ], [ 0.50760835, 0.50047945, 0.40304998, ..., -0.07607754, -0.35598935, 0. ], ..., [ 1.16244466, 1.15544092, 0.85591931, ..., -0.07607754, -0.34906088, 0. ], [-1.13519249, -1.11536499, -0.6299657 , ..., -0.07607754, 0.6887011 , 0. ], [ 1.35264446, 1.35535277, 0.26393325, ..., -0.07607754, 3.15726473, 0. ]]) Step 2: Create a Covariance Matrix covar_matrix = PCA ( n_components = 20 ) #we have 20 features Step 3: Calculate Eigenvalues covar_matrix . fit ( x ) variance = covar_matrix . explained_variance_ratio_ #calculate variance ratios var = np . cumsum ( np . round ( covar_matrix . explained_variance_ratio_ , decimals = 3 ) * 100 ) var #cumulative sum of variance explained with [n] features array([ 33. , 58.9, 68.8, 75.8, 81.6, 86.7, 91.8, 95.3, 97.2, 98.4, 99.4, 99.8, 100.1, 100.1, 100.1, 100.1, 100.1, 100.1, 100.1, 100.1]) In the above array we see that the first feature explains roughly 33% of the variance within our data set while the first two explain 58.9 and so on. If we employ 10 features we capture 98.4% of the variance within the dataset, thus we gain very little by implementing an additional feature (think of this as diminishing marginal return on total variance explained). Step 4, 5 & 6: Sort & Select plt . ylabel ( '% Variance Explained' ) plt . xlabel ( '# of Features' ) plt . title ( 'PCA Analysis' ) plt . ylim ( 30 , 100.5 ) plt . style . context ( 'seaborn-whitegrid' ) plt . plot ( var ) [<matplotlib.lines.Line2D at 0x11666a910>] Based on the plot above it's clear we should pick 10 features.","tags":"Python","loc":"https://etav.github.io/python/scikit_pca.html","title":"Principle Component Analysis (PCA) with Scikit-Learn"},{"url":"https://etav.github.io/python/scatter_plot_python_seaborn.html","text":"Scatter Plot using Seaborn One of the handiest visualization tools for making quick inferences about relationships between variables is the scatter plot. We're going to be using Seaborn and the boston housing data set from the Sci-Kit Learn library to accomplish this. import pandas as pd import seaborn as sb % matplotlib inline from sklearn import datasets import matplotlib.pyplot as plt sb . set ( font_scale = 1.2 , style = \"ticks\" ) #set styling preferences dataset = datasets . load_boston () #convert to pandas data frame df = pd . DataFrame ( dataset . data , columns = dataset . feature_names ) df [ 'target' ] = dataset . target df . head () df = df . rename ( columns = { 'target' : 'median_value' , 'oldName2' : 'newName2' }) df . DIS = df . DIS . round ( 0 ) Describe the data df . describe () . round ( 1 ) CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT median_value count 506.0 506.0 506.0 506.0 506.0 506.0 506.0 506.0 506.0 506.0 506.0 506.0 506.0 506.0 mean 3.6 11.4 11.1 0.1 0.6 6.3 68.6 3.8 9.5 408.2 18.5 356.7 12.7 22.5 std 8.6 23.3 6.9 0.3 0.1 0.7 28.1 2.1 8.7 168.5 2.2 91.3 7.1 9.2 min 0.0 0.0 0.5 0.0 0.4 3.6 2.9 1.0 1.0 187.0 12.6 0.3 1.7 5.0 25% 0.1 0.0 5.2 0.0 0.4 5.9 45.0 2.0 4.0 279.0 17.4 375.4 7.0 17.0 50% 0.3 0.0 9.7 0.0 0.5 6.2 77.5 3.0 5.0 330.0 19.0 391.4 11.4 21.2 75% 3.6 12.5 18.1 0.0 0.6 6.6 94.1 5.0 24.0 666.0 20.2 396.2 17.0 25.0 max 89.0 100.0 27.7 1.0 0.9 8.8 100.0 12.0 24.0 711.0 22.0 396.9 38.0 50.0 Variable Key Variable Name CRIM per capita crime rate by town ZN proportion of residential land zoned for lots over 25,000 sq.ft. INDUS proportion of non-retail business acres per town CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) NOX nitric oxides concentration (parts per 10 million) RM average number of rooms per dwelling AGE proportion of owner-occupied units built prior to 1940 DIS weighted distances to five Boston employment centres RAD index of accessibility to radial highways TAX full-value property-tax rate per \\$10,000 PTRATIO pupil-teacher ratio by town B 1000(Bk - 0.63)&#94;2 where Bk is the proportion of blacks by town LSTAT % lower status of the population median_value Median value of owner-occupied homes in $1000's via UCI Barebones scatter plot plot = sb . lmplot ( x = \"RM\" , y = \"median_value\" , data = df ) Add some color and re-label points = plt . scatter ( df [ \"RM\" ], df [ \"median_value\" ], c = df [ \"median_value\" ], s = 20 , cmap = \"Spectral\" ) #set style options #add a color bar plt . colorbar ( points ) #set limits plt . xlim ( 3 , 9 ) plt . ylim ( 0 , 50 ) #build the plot plot = sb . regplot ( \"RM\" , \"median_value\" , data = df , scatter = False , color = \".1\" ) plot = plot . set ( ylabel = 'Median Home Price ($1000s)' , xlabel = 'Mean Number of Rooms' ) #add labels","tags":"Python","loc":"https://etav.github.io/python/scatter_plot_python_seaborn.html","title":"Scatter Plot in Python using Seaborn"},{"url":"https://etav.github.io/python/pairs_plot_python_seaborn.html","text":"Creating a Pairs Plot using Python One of my favorite functions in R is the pairs plot which makes high-level scatter plots to capture relationships between multiple variables within a dataframe. To my knowledge, python does not have any built-in functions which accomplish this so I turned to Seaborn , the statistical visualization library built on matplotlib, to accomplish this. #import seaborn import seaborn as sb sb . set ( font_scale = 1.35 , style = \"ticks\" ) #set styling preferences % matplotlib inline Load the example iris dataset iris = sb . load_dataset ( \"iris\" ) iris . head () sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa Look at a summary of the data iris . describe () sepal_length sepal_width petal_length petal_width count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.057333 3.758000 1.199333 std 0.828066 0.435866 1.765298 0.762238 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 plot = sb . pairplot ( iris , hue = 'species' ) Change the bar plot to lines graphs. plot = sb . pairplot ( iris , hue = 'species' , diag_kind = 'kde' ) Change the palette of the plot. plot = sb . pairplot ( iris , hue = 'species' , diag_kind = 'kde' , palette = 'husl' )","tags":"Python","loc":"https://etav.github.io/python/pairs_plot_python_seaborn.html","title":"Pairs Plot in Python using Seaborn"},{"url":"https://etav.github.io/articles/ida_eda_method.html","text":"Phase 1 Initial Analysis Overview of Initial Data Analysis (IDA) 0.1 What is initial data analysis? The most important distinction between the initial data analysis phase (IDA) and the main analysis phase (EDA), is that during IDA phase the analyst refrains from any type of activities which directly answer the original research question. Typically the processes in the initial data analysis phase consist of: data quality checks, outlier detection (and treatment), assumption tests and any necessary transformations or imputations for non-normal or missing data. ( Source ) 0.2 In summary IDA seeks to accomplish the following: Uncover underlying structure of the dataset. Detect outliers and anomalies. Test any necessary underlying assumptions. Treatment of problems (typically through transformations or imputations). jerb 1. Understand Underlying Data Structure 1.1 Check the Quality of the Data (Important this happens first) Descriptive Summary Statistics (numerically) mean median standard deviation size of the data set (number of features & observations) 1.2 Check the Quality of Data Collection Method How was the data collected? Do we have any reason to believe the collection process could lead to systematic errors within the data? 2. Detect Outliers and Anomalies 2.1 Check for outliers within the dataset Numerically: using range, max-min functions or standard deviation (Rule of thumb: > 2 Standard deviations = potential outlier) . Visually: histograms. 2.2 Check the normality of the dataset This can be accomplished by plotting a frequency distribution (histogram) for each feature (given a small number of features) and identify any skewness present. Here the analyst should also make note of any missing or miscoded data and decide how to handle it (typically through dropping, or transforming). 3. Test Underlying Assumptions The assumptions the analyst must make will vary based on the model or type of analysis employed. In this example we use the assumptions for linear regression. 3. 1 Assumptions for linear regression: Check for no multicollinearity (collinearity) Ensure Homoscedasticity Linear Relationship Normally distributed No auto-correlation Source: 4. Treat Potential Problems At this step the analyst decides to impute missing data, or transform non-normal variables. Transforming variables is a key tool for analysts because it enables her to manipulate the shape of the distribution or nature of the relationship while preserving the information captured by the variable because at any time during the analysis the analyst can reverse the transformation and get the original value of the variable. 4.1 Transformations Scenario Action The variables's distribution differs moderately from the original. Perform a square root transformation by squaring the observations (x&#94;2). The data's distribution differs significantly from the original. Perform a log transformation by taking the natural logarithm of the observations (log(x)). The data's distribution differs severe from the original. Perform an inverse transformation by taking the inverse of a variable (1/x). Another variant would be to take the negative inverse (-1/x) The data's distribution differs severely and none of the above actions worked. Perform an ordinal transformation by changing the variable's type. ( Source: Boston College ) 4.2 Treatment of Problems After performing necessary transformations, it's vital that the analyst decides how to deal with potential problems and implement any necessary changes within the data before moving onto the next phase. Scenario Action A variable is not normally distributed. Transform using a technique in table 4.1. The dataset is missing data. The analyst should either neglect or impute ( imputation techniques in R ). Significant outliers present. The analyst should either drop outliers or (depending on the weight they carry) transform the variable. Small sample dataset. Here, the analyst should consider using bootstrapping , or resampling with replacement, to project the structure of the dataset onto a larger number of observations. NANs & Infs present. Sometimes datasets will contain NANs or Infs (likely caused by contaminated observations or division by zero) here the analyst must decide to drop the specific observations, or the variable entirely. Incorrect variable types. This problem is often encountered when dealing with time and monetary variables. Here the analyst should decide to change the variables data type or drop it from the analysis. ( Source: World Library ) Phase 2: Exploratory Data Analysis (EDA) ( Source: Wikimedia ) Overview of Exploratory Data Analysis (EDA) 5.1 What is exploratory data analysis? Exploratory data analysis techniques are designed to for open-minded exploration and not guided by a research question. EDA should not be thought of as an exhaustive set of steps to be strictly followed but rather a mindset or philosophy the analyst brings with her to guide her exploration. The analyst uses EDA techniques to \"tease out\" the underlying structure of the data and manipulate it in ways that will reveal otherwise hidden patterns, relationships and features. EDA techniques are primarily graphical because humans have innate pattern recognition abilities which we utilize to synthesize complex conclusions from visual cues. The goal of EDA is to accomplish the following: Maximize insight into a data set. Understand and rank variables by importance. Determine the optimal trade-offs between parsimonious and accurate models. Determine optimal factor settings (such as tuning priors or making manual adjustments to statistical algorithms). 5.2 EDA Steps: Plot raw data. Plot simple statistics. Position the plots in a manner than maximizes the number of inferences a viewer can make while minimizing the both the time it takes and the \"Data-ink ratio\" used to arrive at these insights. (Optional) Leverage the insights from the graphical and statistical analysis to inform model building activities such as feature selection, transformation, and tuning. Some of these steps may seem redundant but it's important to remember that any transformations or significant adjustments to the dataset should have been carried out in the IDA phase . In the EDA phase we must ensure these adjustments had the intended effect and do not contaminate or misrepresent the dataset. ( Source: Engineering Statistics Handbook ) 6. Plotting Raw Data 6.1 Bivaraite Analysis Re-examine the frequency distribution of each variable (for a small number of variables) and ensure they meet expectations. If transformation or imputation has been performed on the variable, note the effect. Histograms 6.2 Multivariate Analysis Begin to examine the nature of the relationships between variables within the dataset. Here the analyst should note any strong relationships (linear or otherwise) or potentially confounding variables which will inform variable selection if model building is the intended goal. Scatter Plots Pairs Plots (Python: seaborn.pairs() , R: pairs() ) 7. Plotting Simple Statistics 7.1 Simple Summary Statistics Similiar to step 1.1, except we must re-examine the summary statistics for transformed or imputed variables, except this time with an emphasis on analyzing the information visually. Mean Median Standard deviation Box plots (or quantile plots) Size of the data set (number of features & observations) Especially if observations or features were dropped or bootstrapped in phase 4.2. Helpful Functions Python pandas.describe function ( docs ) R summary function ( docs ) 8. Position & Present 8.1 Presenting the findings At this point in the exploration the analyst should have: 1. An understanding of the underlying data structure (visually & numerically). (1.1, 1.2, 6.1, 7.1) 2. Insight into the relationships between variables. (6.2) 3. The impact of transformations, imputations or any other significant changes to the original dataset. (4.1, 4.2) The analyst should now feel comfortable positioning their plots and summary statistics in a manner than maximizes the number of inferences a viewer can make while minimizing the both the time it takes and the amount of ink used to arrive at these insights . This could be in the form of a dash board, a single visualization, multiple visualizations or any other acceptable medium of presentation with the goal presenting the data in a manner that engages the viewer and challenges them to ponder its impact and implications ( Source: Introduction to Survey Design ). 9. Model Building Model building is typically the final stage of any analysis. It comes last because the analyst should have a deep understanding of the dataset and how the variables relate to one another. Posessing this information will simplify the following. 9.1 Model Selection What is the goal of the model? How big is the data set? (observations) How many features are contained in the dataset? (variables) Does the data lend itself to supervised or unsupervised machine learning methods? 9.2 Model Fitting Break the dataset into training and testing sets (Rule of thumb: 80/20 split) . Feature Selection & Ranking Principal Component Analysis (PCA) Forward Selection Backward Selection Selecting your algorithm there are many . 9.3 Model Validation Fit the model you selected in 9.1 to your training set and run the testing set through the model to determine its error rate. This rate will differ between modeling techniques. Perform model tuning by adjusting for things like type I and type II error, re-fitting the model after each adjustment. 9.4 EDA Conclusion At the end of the EDA process, the analyst should have the following: - Treatment for outliers and abnormal data. - A ranked list of a dataset's features and how they contribute to the model. - Conclusions or insights arrived at as a result of the exploratory analysis. - A statistically sound and parsimonious model. Guding EDA Questions What is a typical value for a given feature within a dataset? How is the data distributed? Does a given factor have a significant effect on an outcome? What are the most important factors for predicting a given response variable? What is the best model for forecasting unknown values of a response variable? What is signal vs noise in time series data? Does the dataset have outliers?","tags":"Articles","loc":"https://etav.github.io/articles/ida_eda_method.html","title":"Initial & Exploratory Analysis Method"},{"url":"https://etav.github.io/algorithms/knn_implementation_scikit-learn.html","text":"K-Nearest Neighbors Resources: Wikipedia SciKit-Learn StatSoft Definition K-Nearest Neighbors Algorithm (aka kNN) can be used for both classification (data with discrete variables) and regression (data with continuous labels). The algorithm functions by calculating the distance (Sci-Kit Learn uses the formula for Euclidean distance but other formulas are available) between instances to create local \"neighborhoods\". K-Nearest Neighbors functions by maximizing the homogeneity amongst instances within a neighborhood while also maximizing the heterogeneity of instances between neighborhoods. So each member of a given neighborhood \"looks like\" (has similar variables) all of the other members of that neighborhood. One neat feature of the K-Nearest Neighbors algorithm is the number of neighborhoods can be user defined or generated by the algorithm using the local density of points. The Scikit—Learn Function: sklearn.neighbors accepts numpy arrays or scipy.sprace matrices are inputs. For this implementation I will use the classic 'iris data set' included within scikit-learn as a toy data set. Overview of the Data % matplotlib inline import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sb from sklearn.linear_model import LinearRegression from scipy import stats import pylab as pl /Users/ernestt/venv/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment. warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.') from sklearn.datasets import load_iris data = load_iris () print 'Keys:' , data . keys () print '-' * 20 print 'Data Shape:' , data . data . shape print '-' * 20 print 'Features:' , data . feature_names print '-' * 20 Keys: ['target_names', 'data', 'target', 'DESCR', 'feature_names'] -------------------- Data Shape: (150, 4) -------------------- Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] -------------------- What the hell is a Sepal? Personally, I had no idea what a sepal was so I looked up some basic flower anatonmy, I found this picture helpful for relating petal and sepal length. Flower Anatomy: Scatter Plots #Petal Length vs Sepal Width plt . scatter ( data . data [:, 1 ], data . data [:, 2 ], c = data . target , cmap = plt . cm . get_cmap ( 'Set1' , 3 )) plt . xlabel ( data . feature_names [ 1 ]) plt . ylabel ( data . feature_names [ 2 ]) color_bar_formating = plt . FuncFormatter ( lambda i , * args : data . target_names [ int ( i )]) plt . colorbar ( ticks = [ 0 , 1 , 2 ], format = color_bar_formating ) <matplotlib.colorbar.Colorbar at 0x10d23fa50> #Petal Length vs Sepal Width plt . scatter ( data . data [:, 2 ], data . data [:, 3 ], c = data . target , cmap = plt . cm . get_cmap ( 'Set1' , 3 )) plt . xlabel ( data . feature_names [ 2 ]) plt . ylabel ( data . feature_names [ 3 ]) color_bar_formating = plt . FuncFormatter ( lambda i , * args : data . target_names [ int ( i )]) plt . colorbar ( ticks = [ 0 , 1 , 2 ], format = color_bar_formating ) <matplotlib.colorbar.Colorbar at 0x10d505fd0> This plot indicates a strong positive correlation between petal length and width for each of the three flower species import pandas as pd iris_data = pd . read_csv ( 'iris-data.csv' ) iris_data . loc [ iris_data [ 'class' ] == 'versicolor' , 'class' ] = 'Iris-versicolor' #clean species labels iris_data . loc [ iris_data [ 'class' ] == 'Iris-setossa' , 'class' ] = 'Iris-setosa' sb . pairplot ( iris_data . dropna (), hue = 'class' ) #too many species <seaborn.axisgrid.PairGrid at 0x1115a3250> Implementing K-Nearest Neighbors Classifier In this example we're using kNN as a classifier to identify what species a given flower most likely belongs to, given the following four features (measured in cm): sepal length sepal width petal length petal width Essentially this is what is happening under the hood: 1. We use our data to train The kNN Classifier. * This will allow the algorithm to define new neighborhoods. 2. Once the neighborhoods are defined, our classifier will be able to ingest feature data (petal and sepal measurements) on flowers it has not been trained on and determine which neighborhood it is most homogenous to. * Once the neighborhoods have been defined we can actually use the classifier in a generalizable fashion on new data. 3. We can determine the accuracy (and usefulness) of our model by seeing how many flowers it accurately classifies on a testing data set. * In order to do this the actual species must be known. from sklearn import neighbors , datasets # where X = measurements and y = species X , y = data . data , data . target #define the model knn = neighbors . KNeighborsClassifier ( n_neighbors = 5 , weights = 'uniform' ) #fit/train the new model knn . fit ( X , y ) #What species has a 2cm x 2cm sepal and a 4cm x 2cm petal? X_pred = [ 2 , 2 , 4 , 2 ] output = knn . predict ([ X_pred ,]) #use the model we just created to predict print 'Predicted Species:' , data . target_names [ output ] print 'Options:' , data . target_names print 'Probabilities:' , knn . predict_proba ([ X_pred , ]) Predicted Species: ['versicolor'] Options: ['setosa' 'versicolor' 'virginica'] Probabilities: [[ 0. 0.8 0.2]] kNN Decision Boundary Plot Here's a graphical representation of the classifier we created above. As we can see from this plot, the virgincia species is relatively easier to classify when compared to versicolor and setosa. kNN Plot Conclusion The number of neighbors to implement is highly data-dependent meaning optimal neighborhood sizes will differ greatly between data sets. It is important to select a classifier which balances generalizability (precision) and accuracy or we are at risk of overfitting. For example, if we pick a classifier which fits the data perfectly we will lose the ability to make generalizable inferences from it (this would look like the 'low accuracy', 'high precision' scenario below because our model is very good at predicting training data but misses completely when presented with new data). Best practice is to test multiple classifiers using a testing data set to ensure we're making appropriate trade-offs between accuracy and generalizability. We're shooting for high-accuracy and high-precision","tags":"Algorithms","loc":"https://etav.github.io/algorithms/knn_implementation_scikit-learn.html","title":"K-Nearest Neighbors Implementation using Scikit-Learn"},{"url":"https://etav.github.io/python/importing_csv_into_pandas.html","text":"Import necessary modules import pandas as pd import numpy as np Create a toy dataframe (to be converted into csv) data = { 'name' :[ 'Ernest' , 'Jason' , 'Kevin' , 'Christine' ], 'job' :[ 'Analyst' , 'Nerd' , 'Teacher' , 'Product Manager' ], 'salary' :[ '$60,000' , '' , '$70,000' , '$80,000' ]} df = pd . DataFrame ( data , columns = [ 'name' , 'job' , 'salary' ]) df name job salary 0 Ernest Analyst $60,000 1 Jason Nerd 2 Kevin Teacher $70,000 3 Christine Product Manager $80,000 Export the dataframe to a csv in the current directory df . to_csv ( 'career_info.csv' ) Now, load the csv df = pd . read_csv ( 'career_info.csv' ) df Unnamed: 0 name job salary 0 0 Ernest Analyst $60,000 1 1 Jason Nerd NaN 2 2 Kevin Teacher $70,000 3 3 Christine Product Manager $80,000 Notice Pandas conveniently pulls in the header information without needing specification","tags":"Python","loc":"https://etav.github.io/python/importing_csv_into_pandas.html","title":"Importing a CSV Into Pandas"},{"url":"https://etav.github.io/articles/machine_learning_supervision_optional.html","text":"Machine learning is defined as a subfield of computer science and artificial intelligence which \"gives computers the ability to learn without being explicitly programmed\" (source) . Although the statistical techniques which underpin machine learning have existed for decades recent developments in technology such as the availability/affordability of cloud computing and the ability to store and manipulate big data have accelerated its adoption. This essay is meant to explore the most popular methods currently being employed by data scientists such as supervised and unsupervised methods to people with little to no understanding of the field. (An example of a support vector machine (SVM) algorithm being used to create a decision boundary (via wikipedia ) Supervised Supervised machine learning describes an instance where inputs along with the outputs are known. We know the beginning and the end of the story and the challenge is to find a function (story teller, if you will) which best approximates the output in a generalizable fashion. Example : Imagine a doctor trying to predict whether someone has HIV. He has the test results (outputs) and medical records (variables) for patients who have tested positive and negative for the disease. His task is to look at the records and develop a decisioning system so that when a new patient arrives, given just their medical record (variables) he can accurately predict whether or not they are HIV positive. (A graphic represention a logistic regression) Unsupervised Unsupervised machine learning is a bit more abstract because it describes a scenario in which only know the input variables are known but nothing about the outputs are known. A typical task for this type of machine learning is clustering , or grouping the data by using features, to arrive at generalizable insights. Clustering using K-Nearest Neighbors algorithm Semi-Supervised Semi-supervised learning is a hybrid of supervised and unsupervised machine learning . This describes a scenario where there is a small portion of labeled data mixed with unlabeled data. Acquiring labeled data is costly because typically it requires manual input from humans to generate. Semi-supervised learning allows for quicker turn-around while sacrificing accuracy (increasing labeled data increases model accuracy) but it is usually more accurate than unsupervised learning alone. Reinforcement learning Reinforcement learning , or simulation based optimization is a relatively lesser known branch of machine learning but where the future of AI is headed because it requires almost no human input . It's also what enabled Google's Alpha Go to be so successful—and which we'll use for illustrative purposes (here's the research paper ) . Reinforcement learning describes a situation in which humans provide the following: An environment (A Go board) A set of rules (The rules of Go) A Set of actions (All the actions that can be taken) A set of outcomes (Win or lose) Then given an environment, rules, actions and outcomes a computer can repeatedly simulate many, many games of Go and \"learn\" (optimzie) for what strategies work best in a given scenario. What makes reinforcement learning extremely powerful is the sheer number of times a computer can simulate unique games. By the time Alpha Go faced Lee Sedol, the top player in the world, it had simulated more games than Sedol could've ever hoped to play in his lifetime. Humans need to eat, sleep and take breaks, computers don't they just require electricity. There you have it, a quick and dirty overview of some of the more popular machine learning methods currently being employed. Follow me on medium to show your support!","tags":"Articles","loc":"https://etav.github.io/articles/machine_learning_supervision_optional.html","title":"Machine Learning Supervision Optional"},{"url":"https://etav.github.io/projects/random_forest_animal_shelter.html","text":"Animal Shelter The Dataset Some R code for a kaggle competition I entered. The goal was to create a classifier to predict the outcome of a sheltered animal using features such the animal's gender, age and breed. The training dataset contains 26,729 observations, 9 predictor variables and was given to us by the Austin Animal Shelter. Exploratory Analysis Before I dive into creating a classifier, I typically perform an exploratory analysis moving from observing one univariate statistics to bivariate statistics and finally model building. However, I broke from my normal process as curiosity got the best of me. I was interested in learning about what the typical outcomes are for sheltered animals (check out the graph below). Luckily, as we see above, many animals are either adopted, transferred or in the case of dogs frequently returned to their owners. The Variables [ 1 ] \"ID\" \"Name\" \"DateTime\" \"OutcomeType\" \"OutcomeSubtype\" \"AnimalType\" [ 7 ] \"SexuponOutcome\" \"AgeuponOutcome\" \"Breed\" \"Color\" Variable Name Description ID The animal's unique ID. Name The animal's name, if known (many are not). DateTime The date and time the animal entered the shelter (ranges from 1/1/14 - 9/9/15). OutcomeType A five factor variable detailing the outcome for the animal (ie: adopted,transferred, died). OutcomeSubtype 17 factor variable containing Further details related to the outcome of the animal, such as whether or not they were aggressive. AnimalType Whether the animal is a cat or dog. SexuponOutcome The sex of the animal at the time the outcome was recorded. AgeuponOutcome The age of the animal when the outcome was recorded. Breed The breed of the animal (contains mixed breed). Color A Description of the coloring on the animal. Transforming Variables The first thing I did was transform the date variable by separating time and date so that I can analyze them independently, I'd like to be able to compare time of day and any seasonality effects on adoption. I then moved on to address missing name values (there were a few mis-codings which caused errors). After that I moved onto transforming the \"AgeuponOutcome\" variable so that the reported age of animals would all be in the same units, I chose days. This took some chaining of ifelse statements: Animal's Age #Animal Age split <- str_split_fixed ( train $ AgeuponOutcome , \" \" , 2 ) # split value and unit of time split [, 2 ] <- gsub ( \"s\" , \"\" , split [, 2 ]) #remove tailing \"s\" #create a vector to multiply multiplier <- ifelse ( split [, 2 ] == 'day' , 1 , ifelse ( split [, 2 ] == 'week' , 7 , ifelse ( split [, 2 ] == 'month' , 30 , ifelse ( split [, 2 ] == 'year' , 365 , NA )))) train $ days_old <- as.numeric ( split [, 1 ]) * multiplier #apply the multiplier train $ days_old [ 1 : 5 ] #compare, looks good train $ AgeuponOutcome [ 1 : 5 ] After this transformation, we're able to create a visualization which tells us the outcome of each animal type as a function of its age (in days). Interestingly the likelihood of being adopted for cats varies with age whereas for dogs there appears to be a slight negative correlation between a it's age and the probability it will be adopted. For dogs, it seems older animals tend to have a higher likelihood of being returned to their owner (I assume this is has to do with the proliferation of chips for animals) Animal's Gender Moving on I decided to compare the differences in outcomes based on the animal's gender. It's clear that adopters favor animals (both cats and dogs) that have previously been neutered. It's interesting to note that a large proportion of cats which were not neutered are transferred to another animal shelter, where (my guess is) they are then neutered. Applying Random Forest After transforming our variables, performing univariate analysis and determining the validity of our sample, it's finally time to move to model building. I will create a random forest using the RandomForest package, using OutcomeType as our predictor variable (remember there are five levels, which complicates things a bit). rf1 <- randomForest ( OutcomeType ~ AnimalType + SexuponOutcome + Named + days_old + young + color_simple , data = train , importance = T , ntree = 500 , na.action = na.fail ) rf1 Our random forest model does poorly at classifying animal deaths which makes sense when we consider only 197/26729 or 0.007370272% of our training set were flagged as \"Died\". The model does fairly well at predicting instances where an adoption, transfer, or euthanasia occurs which make up the bulk of the training set. Furthermore, our OOB or out of bag error estimate is 35.28%. Here's a detailed breakdown of our Random Forest: #Calling Random Forest Call : randomForest ( formula = OutcomeType ~ AnimalType + SexuponOutcome + Named + days_old + young + color_simple , data = train , importance = T , ntree = 500 , na.action = na.fail ) Type of random forest : classification Number of trees : 500 No. of variables tried at each split : 2 OOB estimate of error rate : 35.28 % Confusion matrix : Adoption Died Euthanasia Return_to_owner Transfer class.error Adoption 8988 0 2 1391 388 0.1653821 Died 21 0 10 10 156 1.0000000 Euthanasia 229 0 175 380 771 0.8874598 Return_to_owner 1998 0 9 2393 386 0.5000000 Transfer 2641 0 83 954 5744 0.3903630 Determining Variable Importance Finally, we'll rank our predictor variables based on their mean reduction in Gini error. As we see in the graphic above, the animal's sex and age were most useful to reduce the mean Gini Error. Interestingly, animal type (ie: cat or dog) and the physical features of the animal such as the color mattered less. That's all for this one folks, thanks for tuning in!","tags":"Projects","loc":"https://etav.github.io/projects/random_forest_animal_shelter.html","title":"Animal Shelter Classifier using Random Forest"}]}