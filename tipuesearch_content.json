{"pages":[{"url":"https://etav.github.io/pages/about.html","text":"I am an aspiring data scientist with a passion for technology startups—I hope to one day found an AI/Machine Learning venture in Silicon Valley. Currently I work at SocialCode as an Advertising Manager but prior to that I was a student at UPenn where I became the first person in my family to earn a college degree (B.S. in Economics) with a triple concentration in Analytics, Marketing and Entrepreneurship from The Wharton School. I never considered myself a \"quant\" but during my time at Penn I challenged myself by taking as many statistics classes as my major would allow and fell in love with the subject. I also interned at 3 Philadelphia startups and DreamIt Ventures before co-founding a startup incubator, WeissLabs during my final year. My first exposure to coding was R but currently I'm learning Python. Email: hi@ernesttavares.co Twitter: @etav Resumé Education B.S., Economics , The Wharton School, University of Pennsylvania, Philadelphia, PA. 2016 Triple concentration in analytics, marketing and entrepreneurship, philosophy (minor) Experience Associate Advertising Manager , SocialCode , 2015 - Present Managing daily optimization and execution across all aspects of social advertising campaigns, ensuring client goals are met in a timely and efficient manner. Co-Founder , WeissLabs , 2015 - 2016 Responsible for all aspects of forming an student-run organization including: Initial team formation Sourcing student-run startups Developing a 7-week entrepreneurship curriculum Connecting startups with investors Business Analyst Intern , Leadnomics , 2014 - 2016 Leveraged operational data to create reports to inform business strategy. Inventoried machine learning software (such as: Google Tensorflow, vowpal wabbit, sci-kit learn) to evaluate accuracy and usability. Strategic Investment Intern , DreamIt Ventures , 2015 - 2015 Identified and contacted venture capitalist and angel investors who had a history of investing in early-stage healthcare startups.","tags":"pages","loc":"https://etav.github.io/pages/about.html","title":"About Ernest Tavares"},{"url":"https://etav.github.io/python/scatter_plot_python_seaborn.html","text":"Scatter Plot using Seaborn One of the handiest visualization tools for making quick inferences about relationships between variables is the scatter plot. We're going to be using Seaborn and the boston housing data set from the Sci-Kit Learn library to accomplish this. import pandas as pd import seaborn as sb % matplotlib inline from sklearn import datasets import matplotlib.pyplot as plt sb . set ( font_scale = 1.2 , style = \"ticks\" ) #set styling preferences dataset = datasets . load_boston () #convert to pandas data frame df = pd . DataFrame ( dataset . data , columns = dataset . feature_names ) df [ 'target' ] = dataset . target df . head () df = df . rename ( columns = { 'target' : 'median_value' , 'oldName2' : 'newName2' }) df . DIS = df . DIS . round ( 0 ) Describe the data df . describe () . round ( 1 ) CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT median_value count 506.0 506.0 506.0 506.0 506.0 506.0 506.0 506.0 506.0 506.0 506.0 506.0 506.0 506.0 mean 3.6 11.4 11.1 0.1 0.6 6.3 68.6 3.8 9.5 408.2 18.5 356.7 12.7 22.5 std 8.6 23.3 6.9 0.3 0.1 0.7 28.1 2.1 8.7 168.5 2.2 91.3 7.1 9.2 min 0.0 0.0 0.5 0.0 0.4 3.6 2.9 1.0 1.0 187.0 12.6 0.3 1.7 5.0 25% 0.1 0.0 5.2 0.0 0.4 5.9 45.0 2.0 4.0 279.0 17.4 375.4 7.0 17.0 50% 0.3 0.0 9.7 0.0 0.5 6.2 77.5 3.0 5.0 330.0 19.0 391.4 11.4 21.2 75% 3.6 12.5 18.1 0.0 0.6 6.6 94.1 5.0 24.0 666.0 20.2 396.2 17.0 25.0 max 89.0 100.0 27.7 1.0 0.9 8.8 100.0 12.0 24.0 711.0 22.0 396.9 38.0 50.0 Variable Key Variable Name CRIM per capita crime rate by town ZN proportion of residential land zoned for lots over 25,000 sq.ft. INDUS proportion of non-retail business acres per town CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) NOX nitric oxides concentration (parts per 10 million) RM average number of rooms per dwelling AGE proportion of owner-occupied units built prior to 1940 DIS weighted distances to five Boston employment centres RAD index of accessibility to radial highways TAX full-value property-tax rate per \\$10,000 PTRATIO pupil-teacher ratio by town B 1000(Bk - 0.63)&#94;2 where Bk is the proportion of blacks by town LSTAT % lower status of the population median_value Median value of owner-occupied homes in $1000's via UCI Barebones scatter plot plot = sb . lmplot ( x = \"RM\" , y = \"median_value\" , data = df ) Add some color and re-label points = plt . scatter ( df [ \"RM\" ], df [ \"median_value\" ], c = df [ \"median_value\" ], s = 20 , cmap = \"Spectral\" ) #set style options #add a color bar plt . colorbar ( points ) #set limits plt . xlim ( 3 , 9 ) plt . ylim ( 0 , 50 ) #build the plot plot = sb . regplot ( \"RM\" , \"median_value\" , data = df , scatter = False , color = \".1\" ) plot = plot . set ( ylabel = 'Median Home Price ($1000s)' , xlabel = 'Mean Number of Rooms' ) #add labels","tags":"Python","loc":"https://etav.github.io/python/scatter_plot_python_seaborn.html","title":"Scatter Plot in Python using Seaborn"},{"url":"https://etav.github.io/python/pairs_plot_python_seaborn.html","text":"Creating a Pairs Plot using Python One of my favorite functions in R is the pairs plot which makes high-level scatter plots to capture relationships between multiple variables within a dataframe. To my knowledge, python does not have any built-in functions which accomplish this so I turned to Seaborn , the statistical visualization library built on matplotlib, to accomplish this. #import seaborn import seaborn as sb sb . set ( font_scale = 1.35 , style = \"ticks\" ) #set styling preferences % matplotlib inline Load the example iris dataset iris = sb . load_dataset ( \"iris\" ) iris . head () sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa Look at a summary of the data iris . describe () sepal_length sepal_width petal_length petal_width count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.057333 3.758000 1.199333 std 0.828066 0.435866 1.765298 0.762238 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 plot = sb . pairplot ( iris , hue = 'species' ) Change the bar plot to lines graphs. plot = sb . pairplot ( iris , hue = 'species' , diag_kind = 'kde' ) Change the palette of the plot. plot = sb . pairplot ( iris , hue = 'species' , diag_kind = 'kde' , palette = 'husl' )","tags":"Python","loc":"https://etav.github.io/python/pairs_plot_python_seaborn.html","title":"Pairs Plot in Python using Seaborn"},{"url":"https://etav.github.io/algorithms/knn_implementation_scikit-learn.html","text":"K-Nearest Neighbors Resources: Wikipedia SciKit-Learn StatSoft Definition K-Nearest Neighbors Algorithm (aka kNN) can be used for both classification (data with discrete variables) and regression (data with continuous labels). The algorithm functions by calculating the distance (Sci-Kit Learn uses the formula for Euclidean distance but other formulas are available) between instances to create local \"neighborhoods\". K-Nearest Neighbors functions by maximizing the homogeneity amongst instances within a neighborhood while also maximizing the heterogeneity of instances between neighborhoods. So each member of a given neighborhood \"looks like\" (has similar variables) all of the other members of that neighborhood. One neat feature of the K-Nearest Neighbors algorithm is the number of neighborhoods can be user defined or generated by the algorithm using the local density of points. The Scikit—Learn Function: sklearn.neighbors accepts numpy arrays or scipy.sprace matrices are inputs. For this implementation I will use the classic 'iris data set' included within scikit-learn as a toy data set. Overview of the Data % matplotlib inline import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sb from sklearn.linear_model import LinearRegression from scipy import stats import pylab as pl /Users/ernestt/venv/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment. warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.') from sklearn.datasets import load_iris data = load_iris () print 'Keys:' , data . keys () print '-' * 20 print 'Data Shape:' , data . data . shape print '-' * 20 print 'Features:' , data . feature_names print '-' * 20 Keys: ['target_names', 'data', 'target', 'DESCR', 'feature_names'] -------------------- Data Shape: (150, 4) -------------------- Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] -------------------- What the hell is a Sepal? Personally, I had no idea what a sepal was so I looked up some basic flower anatonmy, I found this picture helpful for relating petal and sepal length. Flower Anatomy: Scatter Plots #Petal Length vs Sepal Width plt . scatter ( data . data [:, 1 ], data . data [:, 2 ], c = data . target , cmap = plt . cm . get_cmap ( 'Set1' , 3 )) plt . xlabel ( data . feature_names [ 1 ]) plt . ylabel ( data . feature_names [ 2 ]) color_bar_formating = plt . FuncFormatter ( lambda i , * args : data . target_names [ int ( i )]) plt . colorbar ( ticks = [ 0 , 1 , 2 ], format = color_bar_formating ) <matplotlib.colorbar.Colorbar at 0x10d23fa50> #Petal Length vs Sepal Width plt . scatter ( data . data [:, 2 ], data . data [:, 3 ], c = data . target , cmap = plt . cm . get_cmap ( 'Set1' , 3 )) plt . xlabel ( data . feature_names [ 2 ]) plt . ylabel ( data . feature_names [ 3 ]) color_bar_formating = plt . FuncFormatter ( lambda i , * args : data . target_names [ int ( i )]) plt . colorbar ( ticks = [ 0 , 1 , 2 ], format = color_bar_formating ) <matplotlib.colorbar.Colorbar at 0x10d505fd0> This plot indicates a strong positive correlation between petal length and width for each of the three flower species import pandas as pd iris_data = pd . read_csv ( 'iris-data.csv' ) iris_data . loc [ iris_data [ 'class' ] == 'versicolor' , 'class' ] = 'Iris-versicolor' #clean species labels iris_data . loc [ iris_data [ 'class' ] == 'Iris-setossa' , 'class' ] = 'Iris-setosa' sb . pairplot ( iris_data . dropna (), hue = 'class' ) #too many species <seaborn.axisgrid.PairGrid at 0x1115a3250> Implementing K-Nearest Neighbors Classifier In this example we're using kNN as a classifier to identify what species a given flower most likely belongs to, given the following four features (measured in cm): sepal length sepal width petal length petal width Essentially this is what is happening under the hood: 1. We use our data to train The kNN Classifier. * This will allow the algorithm to define new neighborhoods. 2. Once the neighborhoods are defined, our classifier will be able to ingest feature data (petal and sepal measurements) on flowers it has not been trained on and determine which neighborhood it is most homogenous to. * Once the neighborhoods have been defined we can actually use the classifier in a generalizable fashion on new data. 3. We can determine the accuracy (and usefulness) of our model by seeing how many flowers it accurately classifies on a testing data set. * In order to do this the actual species must be known. from sklearn import neighbors , datasets # where X = measurements and y = species X , y = data . data , data . target #define the model knn = neighbors . KNeighborsClassifier ( n_neighbors = 5 , weights = 'uniform' ) #fit/train the new model knn . fit ( X , y ) #What species has a 2cm x 2cm sepal and a 4cm x 2cm petal? X_pred = [ 2 , 2 , 4 , 2 ] output = knn . predict ([ X_pred ,]) #use the model we just created to predict print 'Predicted Species:' , data . target_names [ output ] print 'Options:' , data . target_names print 'Probabilities:' , knn . predict_proba ([ X_pred , ]) Predicted Species: ['versicolor'] Options: ['setosa' 'versicolor' 'virginica'] Probabilities: [[ 0. 0.8 0.2]] kNN Decision Boundary Plot Here's a graphical representation of the classifier we created above. As we can see from this plot, the virgincia species is relatively easier to classify when compared to versicolor and setosa. kNN Plot Conclusion The number of neighbors to implement is highly data-dependent meaning optimal neighborhood sizes will differ greatly between data sets. It is important to select a classifier which balances generalizability (precision) and accuracy or we are at risk of overfitting. For example, if we pick a classifier which fits the data perfectly we will lose the ability to make generalizable inferences from it (this would look like the 'low accuracy', 'high precision' scenario below because our model is very good at predicting training data but misses completely when presented with new data). Best practice is to test multiple classifiers using a testing data set to ensure we're making appropriate trade-offs between accuracy and generalizability. We're shooting for high-accuracy and high-precision","tags":"Algorithms","loc":"https://etav.github.io/algorithms/knn_implementation_scikit-learn.html","title":"K-Nearest Neighbors Implementation using Scikit-Learn"},{"url":"https://etav.github.io/python/importing_csv_into_pandas.html","text":"Import necessary modules import pandas as pd import numpy as np Create a toy dataframe (to be converted into csv) data = { 'name' :[ 'Ernest' , 'Jason' , 'Kevin' , 'Christine' ], 'job' :[ 'Analyst' , 'Nerd' , 'Teacher' , 'Product Manager' ], 'salary' :[ '$60,000' , '' , '$70,000' , '$80,000' ]} df = pd . DataFrame ( data , columns = [ 'name' , 'job' , 'salary' ]) df name job salary 0 Ernest Analyst $60,000 1 Jason Nerd 2 Kevin Teacher $70,000 3 Christine Product Manager $80,000 Export the dataframe to a csv in the current directory df . to_csv ( 'career_info.csv' ) Now, load the csv df = pd . read_csv ( 'career_info.csv' ) df Unnamed: 0 name job salary 0 0 Ernest Analyst $60,000 1 1 Jason Nerd NaN 2 2 Kevin Teacher $70,000 3 3 Christine Product Manager $80,000 Notice Pandas conveniently pulls in the header information without needing specification","tags":"Python","loc":"https://etav.github.io/python/importing_csv_into_pandas.html","title":"Importing a CSV Into Pandas"},{"url":"https://etav.github.io/articles/machine_learning_supervision_optional.html","text":"Machine learning is defined as a subfield of computer science and artificial intelligence which \"gives computers the ability to learn without being explicitly programmed\" (source) . Although the statistical techniques which underpin machine learning have existed for decades recent developments in technology such as the availability/affordability of cloud computing and the ability to store and manipulate big data have accelerated its adoption. This essay is meant to explore the most popular methods currently being employed by data scientists such as supervised and unsupervised methods to people with little to no understanding of the field. (An example of a support vector machine (SVM) algorithm being used to create a decision boundary (via wikipedia ) Supervised Supervised machine learning describes an instance where inputs along with the outputs are known. We know the beginning and the end of the story and the challenge is to find a function (story teller, if you will) which best approximates the output in a generalizable fashion. Example : Imagine a doctor trying to predict whether someone has HIV. He has the test results (outputs) and medical records (variables) for patients who have tested positive and negative for the disease. His task is to look at the records and develop a decisioning system so that when a new patient arrives, given just their medical record (variables) he can accurately predict whether or not they are HIV positive. (A graphic represention a logistic regression) Unsupervised Unsupervised machine learning is a bit more abstract because it describes a scenario in which only know the input variables are known but nothing about the outputs are known. A typical task for this type of machine learning is clustering , or grouping the data by using features, to arrive at generalizable insights. Clustering using K-Nearest Neighbors algorithm Semi-Supervised Semi-supervised learning is a hybrid of supervised and unsupervised machine learning . This describes a scenario where there is a small portion of labeled data mixed with unlabeled data. Acquiring labeled data is costly because typically it requires manual input from humans to generate. Semi-supervised learning allows for quicker turn-around while sacrificing accuracy (increasing labeled data increases model accuracy) but it is usually more accurate than unsupervised learning alone. Reinforcement learning Reinforcement learning , or simulation based optimization is a relatively lesser known branch of machine learning but where the future of AI is headed because it requires almost no human input . It's also what enabled Google's Alpha Go to be so successful—and which we'll use for illustrative purposes (here's the research paper ) . Reinforcement learning describes a situation in which humans provide the following: An environment (A Go board) A set of rules (The rules of Go) A Set of actions (All the actions that can be taken) A set of outcomes (Win or lose) Then given an environment, rules, actions and outcomes a computer can repeatedly simulate many, many games of Go and \"learn\" (optimzie) for what strategies work best in a given scenario. What makes reinforcement learning extremely powerful is the sheer number of times a computer can simulate unique games. By the time Alpha Go faced Lee Sedol, the top player in the world, it had simulated more games than Sedol could've ever hoped to play in his lifetime. Humans need to eat, sleep and take breaks, computers don't they just require electricity. There you have it, a quick and dirty overview of some of the more popular machine learning methods currently being employed. Follow me on medium to show your support!","tags":"Articles","loc":"https://etav.github.io/articles/machine_learning_supervision_optional.html","title":"Machine Learning Supervision Optional"},{"url":"https://etav.github.io/projects/random_forest_animal_shelter.html","text":"Animal Shelter The Dataset Some R code for a kaggle competition I entered. The goal was to create a classifier to predict the outcome of a sheltered animal using features such the animal's gender, age and breed. The training dataset contains 26,729 observations, 9 predictor variables and was given to us by the Austin Animal Shelter. Exploratory Analysis Before I dive into creating a classifier, I typically perform an exploratory analysis moving from observing one univariate statistics to bivariate statistics and finally model building. However, I broke from my normal process as curiosity got the best of me. I was interested in learning about what the typical outcomes are for sheltered animals (check out the graph below). Luckily, as we see above, many animals are either adopted, transferred or in the case of dogs frequently returned to their owners. The Variables [ 1 ] \"ID\" \"Name\" \"DateTime\" \"OutcomeType\" \"OutcomeSubtype\" \"AnimalType\" [ 7 ] \"SexuponOutcome\" \"AgeuponOutcome\" \"Breed\" \"Color\" Variable Name Description ID The animal's unique ID. Name The animal's name, if known (many are not). DateTime The date and time the animal entered the shelter (ranges from 1/1/14 - 9/9/15). OutcomeType A five factor variable detailing the outcome for the animal (ie: adopted,transferred, died). OutcomeSubtype 17 factor variable containing Further details related to the outcome of the animal, such as whether or not they were aggressive. AnimalType Whether the animal is a cat or dog. SexuponOutcome The sex of the animal at the time the outcome was recorded. AgeuponOutcome The age of the animal when the outcome was recorded. Breed The breed of the animal (contains mixed breed). Color A Description of the coloring on the animal. Transforming Variables The first thing I did was transform the date variable by separating time and date so that I can analyze them independently, I'd like to be able to compare time of day and any seasonality effects on adoption. I then moved on to address missing name values (there were a few mis-codings which caused errors). After that I moved onto transforming the \"AgeuponOutcome\" variable so that the reported age of animals would all be in the same units, I chose days. This took some chaining of ifelse statements: Animal's Age #Animal Age split <- str_split_fixed ( train $ AgeuponOutcome , \" \" , 2 ) # split value and unit of time split [, 2 ] <- gsub ( \"s\" , \"\" , split [, 2 ]) #remove tailing \"s\" #create a vector to multiply multiplier <- ifelse ( split [, 2 ] == 'day' , 1 , ifelse ( split [, 2 ] == 'week' , 7 , ifelse ( split [, 2 ] == 'month' , 30 , ifelse ( split [, 2 ] == 'year' , 365 , NA )))) train $ days_old <- as.numeric ( split [, 1 ]) * multiplier #apply the multiplier train $ days_old [ 1 : 5 ] #compare, looks good train $ AgeuponOutcome [ 1 : 5 ] After this transformation, we're able to create a visualization which tells us the outcome of each animal type as a function of its age (in days). Interestingly the likelihood of being adopted for cats varies with age whereas for dogs there appears to be a slight negative correlation between a it's age and the probability it will be adopted. For dogs, it seems older animals tend to have a higher likelihood of being returned to their owner (I assume this is has to do with the proliferation of chips for animals) Animal's Gender Moving on I decided to compare the differences in outcomes based on the animal's gender. It's clear that adopters favor animals (both cats and dogs) that have previously been neutered. It's interesting to note that a large proportion of cats which were not neutered are transferred to another animal shelter, where (my guess is) they are then neutered. Applying Random Forest After transforming our variables, performing univariate analysis and determining the validity of our sample, it's finally time to move to model building. I will create a random forest using the RandomForest package, using OutcomeType as our predictor variable (remember there are five levels, which complicates things a bit). rf1 <- randomForest ( OutcomeType ~ AnimalType + SexuponOutcome + Named + days_old + young + color_simple , data = train , importance = T , ntree = 500 , na.action = na.fail ) rf1 Our random forest model does poorly at classifying animal deaths which makes sense when we consider only 197/26729 or 0.007370272% of our training set were flagged as \"Died\". The model does fairly well at predicting instances where an adoption, transfer, or euthanasia occurs which make up the bulk of the training set. Furthermore, our OOB or out of bag error estimate is 35.28%. Here's a detailed breakdown of our Random Forest: #Calling Random Forest Call : randomForest ( formula = OutcomeType ~ AnimalType + SexuponOutcome + Named + days_old + young + color_simple , data = train , importance = T , ntree = 500 , na.action = na.fail ) Type of random forest : classification Number of trees : 500 No. of variables tried at each split : 2 OOB estimate of error rate : 35.28 % Confusion matrix : Adoption Died Euthanasia Return_to_owner Transfer class.error Adoption 8988 0 2 1391 388 0.1653821 Died 21 0 10 10 156 1.0000000 Euthanasia 229 0 175 380 771 0.8874598 Return_to_owner 1998 0 9 2393 386 0.5000000 Transfer 2641 0 83 954 5744 0.3903630 Determining Variable Importance Finally, we'll rank our predictor variables based on their mean reduction in Gini error. As we see in the graphic above, the animal's sex and age were most useful to reduce the mean Gini Error. Interestingly, animal type (ie: cat or dog) and the physical features of the animal such as the color mattered less. That's all for this one folks, thanks for tuning in!","tags":"Projects","loc":"https://etav.github.io/projects/random_forest_animal_shelter.html","title":"Animal Shelter Classifier using Random Forest"},{"url":"https://etav.github.io/algorithms/rf_test.html","text":"from treeinterpreter import treeinterpreter as ti from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor import matplotlib.pyplot as plt import pandas as pd import numpy as np from sklearn.metrics import mean_squared_error as mse from sklearn.metrics import r2_score from sklearn.datasets import load_boston boston = load_boston () forest = RandomForestRegressor () url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data' df = pd . read_csv ( url , sep = '\\s+' , names = [ 'CRIM' , 'ZN' , 'INDUS' , 'CHAS' , 'NOX' , 'RM' , 'AGE' , 'DIS' , 'RAD' , 'TAX' , 'PTRATIO' , 'B' , 'LSTAT' , 'median_val' ]) /Users/ernestt/venv/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment. warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.') df [ 'median_val' ] = df [ 'median_val' ] * 1000 df_target = df [ 'median_val' ] df_target df_data = df [ 'median_val' ] #plotting housing prices price = df . groupby ( 'median_val' )[ 'median_val' ] . count () plt . figure ( figsize = ( 10 , 5 )) plt . hist ( price . values , bins = 10 , log = False , color = '#F18118' ) plt . xlabel ( 'Median Price ($)' , fontsize = 15 ) plt . ylabel ( 'Count' , fontsize = 15 ) plt . show () #feature importance for boston forest . fit ( boston . data , boston . target ) importance = forest . feature_importances_ std = np . std ([ tree . feature_importances_ for tree in forest . estimators_ ], axis = 0 ) sort = np . argsort ( importance )[:: - 1 ] features = [] # Print the feature ranking print ( \"Feature ranking:\" ) for f in range ( boston . data . shape [ 1 ]): features . append ( \" %d . feature %d ( %f )\" % ( f + 1 , sort [ f ], importance [ sort [ f ]])) print ( \" %d . feature: %d ( %f )\" % ( f + 1 , sort [ f ], importance [ sort [ f ]])) #df= pd.DataFrame(boston.data, columns = ['CRIM', 'ZN', 'INDUS', 'CHAS','NOX','RM', 'AGE', 'DIS','RAD', 'TAX', 'PTRATIO', 'B','LSTAT']) #df # Plot the feature importances of the forest #plt.figure() #plt.title(\"Feature importances\") #plt.bar(range(boston.data[:300].shape[1]), importance[sort], # color=\"r\", yerr=std[sort], align=\"center\") #plt.xticks(range(boston.data[:300].shape[1]), sort) #plt.xlim([-1, boston.data[:300].shape[1]]) #plt.show() #Linear Regression from sklearn.linear_model import LinearRegression model = LinearRegression () model . fit ( boston . data , boston . target ) e = boston . target #expected p = model . predict ( boston . data ) #predicted print \"Linear regression model \\n Boston dataset\" print \"Mean squared error = %0.3f \" % mse ( e , p ) print \"R2 score = %0.3f \" % r2_score ( e , p ) Linear regression model Boston dataset Mean squared error = 21.898 R2 score = 0.741 /Users/ernestt/venv/lib/python2.7/site-packages/scipy/linalg/basic.py:884: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver. warnings.warn(mesg, RuntimeWarning) #Random Forest forest . fit ( boston . data , boston . target ) e = boston . target p = forest . predict ( boston . data ) print \"Random Forest Algorithm on Boston Housing dataset\" print \"Mean squared error = %0.2f \" % mse ( e , p ) #mean square error print \"R2 score = %0.2f \" % r2_score ( e , p ) # #print boston.DESCR Random Forest Algorithm on Boston Housing dataset Mean squared error = 2.85 R2 score = 0.97 #pair-wise plots import matplotlib.pyplot as plt subset = df axes = pd . tools . plotting . scatter_matrix ( df [ 'B' ], alpha =. 05 ) #plt.savefig('scatter_matrix.png') plt . show () #plt.figure() #plt.title(\"Feature importances\") #plt.bar(range(boston.data[:300].shape[1]), importance[sort], # color=\"r\", yerr=std[sort], align=\"center\") --------------------------------------------------------------------------- KeyError Traceback ( most recent call last ) < ipython - input - 30 - d210314bfca8 > in < module > () 2 import matplotlib.pyplot as plt 3 ----> 4 axes = pd . tools . plotting . scatter_matrix ( df [ 'B' , 'median_val' ], alpha =. 05 ) 5 #plt.savefig('scatter_matrix.png') 6 plt . show () / Users / ernestt / venv / lib / python2 . 7 / site - packages / pandas / core / frame . pyc in __getitem__ ( self , key ) 1995 return self . _getitem_multilevel ( key ) 1996 else : -> 1997 return self . _getitem_column ( key ) 1998 1999 def _getitem_column ( self , key ): / Users / ernestt / venv / lib / python2 . 7 / site - packages / pandas / core / frame . pyc in _getitem_column ( self , key ) 2002 # get column 2003 if self . columns . is_unique : -> 2004 return self . _get_item_cache ( key ) 2005 2006 # duplicate columns & possible reduce dimensionality / Users / ernestt / venv / lib / python2 . 7 / site - packages / pandas / core / generic . pyc in _get_item_cache ( self , item ) 1348 res = cache . get ( item ) 1349 if res is None : -> 1350 values = self . _data . get ( item ) 1351 res = self . _box_item_values ( item , values ) 1352 cache [ item ] = res / Users / ernestt / venv / lib / python2 . 7 / site - packages / pandas / core / internals . pyc in get ( self , item , fastpath ) 3288 3289 if not isnull ( item ): -> 3290 loc = self . items . get_loc ( item ) 3291 else : 3292 indexer = np . arange ( len ( self . items ))[ isnull ( self . items )] / Users / ernestt / venv / lib / python2 . 7 / site - packages / pandas / indexes / base . pyc in get_loc ( self , key , method , tolerance ) 1945 return self . _engine . get_loc ( key ) 1946 except KeyError : -> 1947 return self . _engine . get_loc ( self . _maybe_cast_indexer ( key )) 1948 1949 indexer = self . get_indexer ([ key ], method = method , tolerance = tolerance ) pandas / index . pyx in pandas . index . IndexEngine . get_loc ( pandas / index . c : 4154 )() pandas / index . pyx in pandas . index . IndexEngine . get_loc ( pandas / index . c : 4018 )() pandas / hashtable . pyx in pandas . hashtable . PyObjectHashTable . get_item ( pandas / hashtable . c : 12368 )() pandas / hashtable . pyx in pandas . hashtable . PyObjectHashTable . get_item ( pandas / hashtable . c : 12322 )() KeyError : ( 'B' , 'median_val' )","tags":"Algorithms","loc":"https://etav.github.io/algorithms/rf_test.html","title":"Random Forest test"}]}